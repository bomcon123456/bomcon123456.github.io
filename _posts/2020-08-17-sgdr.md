---
layout: category-post
title:  "Paper recap: SGDR, Super-convergence"
date:   2020-08-17
categories: paper-recap
---
## Cyclical Learning Rates for Training Neural Networks
[Lesli N.Smith, 2017](https://arxiv.org/abs/1506.01186)

## Research Topic
- Category (General): Deep Learning
- Category (Specific): Hyper Tuning, Optimization

## Paper summary

### SGDR
    - Partial warm restarts improve rate of convergence, often used in gradient-free optimization.
    - Propose a warm restart technique for stochastic gradient descent.
    - Study its performance on CIFAR-10/100
    - Show that this technique improve its anytime performance when training deep neural network:
        - SGD with warm restarts requires 2× to 4× fewer epochs than the currently-used learning rate schedule schemes to achieve comparable or even better results

### Super-convergence
    - Introduce "Super-convergence" term: where neural networks can be train and converge much faster than standard training methods.
    - Propoce a way to achieve super-convergence: one-cycle policy + large learning rate.
    - Use Hessian Free optimization method to produce an estimate of the optimal learning rate.
    - Study its performance on CIFAR-10/100, MNIST, Imagenet with various model.
    - Show that this phenomenon can also happen when the amount of labeled training data is limited but still boost the model performance.

## Explain Like I'm 5 (ELI5) 👶👶👶

### SGDR
    - Just like a normal person works everyday. You start the day with maximum effort, but then time goes by you feel tired and the productivity reduces. You go home, rest. The next day, you're recharged and start the cycle once again.

### Super-convergence
    - As a result of CLR, super-convergence is born.

## Issues addressed by the paper

### SGDR
    - Despite of the existance of advanced optimization methods like Adam, AdaDelta, SOTA result on CIFAR-10/100, ImageNet still based on SGD with momentum, associated with Resnet model.
    - Current way to get out of the plateau while using SGD is LR scheduler and L2 regularization.
    - They want to break through and produce a new approach to SGD.

### Super-convergence
    - They found a way to train DNN faster and achieve better performance.
    - Large LR regularizes training, so other regularization method should be reduced to maintain optimal balance of optimization
    - Hessian-free optimization method estimates optimal LR, demonstrating that large LR find wide, flat minima.

## Approach/Method

### SGDR
    - SGDR simulates a new warm-started run/restart of SGD after \\(T_{i}\\) epochs are performed.

<p align="center">
<img src="/assets/images/sgdr/cos_annealing.png" style="width: 50%;" alt="cosann">
</p>
    - Learning rate is calculated by cos annealing function.
    
    $$
        \eta_{t}=\eta_{\min }^{i}+\frac{1}{2}\left(\eta_{\max }^{i}-\eta_{\min }^{i}\right)\left(1+\cos \left(\frac{T_{\text {cur}}}{T_{i}} \pi\right)\right)
    $$

    - \\(\eta\\): learning rate.
    - \\(T_{cur}\\): how many epochs passed since the restart.
    - \\(T_{i}\\): how mane epochs for a restart, you can leave it constant or increase over-time.
    
    - How the learning rate looks after training:
<p align="center">
<img src="/assets/images/sgdr/sgdr.png" style="width: 50%;" alt="sgdr">
</p>

### Super-convergence
    -

## Best practice

## Hidden gems💎💎💎


## Results


## Limitations


## Confusing aspects of the paper


## Conclusions

### The author's conclusions


### Rating


### My Conclusion


## Paper implementation

### Cyclical Learning Rates


### Learning Rate Finder


## Cited references and used images from:


## Papers needs to conquer next 👏👏👏
