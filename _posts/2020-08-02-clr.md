---
layout: category-post
title:  "Paper recap: Cyclical Learning Rates for Training Neural Networks"
date:   2020-08-02
categories: paper-recap
---
# Cyclical Learning Rates for Training Neural Networks
[Lesli N.Smith, 2015](https://arxiv.org/abs/1506.01186)
## Research Topic
- Category (General): Deep Learning
- Category (Specific): Hyperparameters (Learning rate) Tuning
## Paper summary
- A new method (called Cyclical Learning Rates) for setting the learning rate, instead of monotonically decreasing it as in the traditional way, allowing the hyper-parameter to rise and fall systematically during training.
- CLR improves classification accuracy without tuning and doesn't require much computations.
- The author also showed a way to estimate the bounds for the learning rate to vary while training.
## Issues addressed by the paper
- To train a deep neural network to convergence requires one to experiment with a variety of LR.
- There are already multiple solutions to this such as learning rate scheduling (e.g: time-based decay, step decay, exponential decay,...), or adaptive learning rate (e.g: RMSProp, AdaGrad, AdaDelta, Adam,...), but there are still some drawbacks such as:
  - Learning rate scheduling: monotonically decreasing learning rate, which later proved by experiences in the paper that it wouldn't help the model to escape from the saddle point.
  - Adaptive learning rate: requires significant computational cost.
## Approach/Method
- Observation: increasing the learning rate might have a short term negative effect and yet achieve a longer-term beneficial effect. (since increasing the learning rate allows more rapid traversal of saddle point plateaus)
- Minimum (`base_lr`) and maximum (`max_lr`) boundaries will be set and then the learning rate will cyclically vary between these bounds.

![Window type](/assets/images/clr/window.png)
- Triangular(Bartlett) window, parabolic(Welch) window and sinusoidal(Hanning) window produced equivalent results, which led to adopting a triangular window thanks to its simplicity.

![Triangular window](/assets/images/clr/triangular.png)

- How to find the bounds? 
  - Using "LR range test": 
    - run model for several epochs while letting the learning rate increase linearly between low and high LR values. Notice
    - Notice these 2 points:
      - When accuracy starts to increase. -> `base_lr`
      - When accuracy slows down, becomes ragged or falls down. -> `max_lr`

![LR_finder](/assets/images/clr/lr_range.png)
## Best practices
- Set `stepsize` to 2-10 times of #iterations/epoch
- Best to stop training at the end of the cycle (LR at `base_lr` and the accuracy peaks) 
  - -> Early stopping might not be good for CLR
- Optimum learning rate is usually within a factor of two of the largest one that converges, and set `base_lr` = \\(\frac{1}{3}\\) or \\(\frac{1}{4}\\)  of `max_lr`
## Results
![Result](/assets/images/clr/clr_result.png)
- `CLR` helps to model to converge much faster
- `Decay` (monotonically decreasing LR)'s result provides evidence that both increasing and decreasing LR are essential.
## Limitations
![Result](/assets/images/clr/clr_adaptive_result.png)
- When using with adaptive learning rate methods, the benefits from CLR are reduced.
## Confusing aspects of the paper
![Triangular code](/assets/images/clr/triangular_code.png)
- The variables of the code weren't explained clearly, so I redefine it here:
  - **1 epoch**: converted to #iterations (= training_size / batch_size)
  - **1 cycle**: LR goes from `base_lr` -> `max_lr` -> `base_lr`, and finish 1 epoch
  - `epochCounter`: current #iterations (e.g: 1 epoch has 2000 iters, the model's at epochs 1.5 => `epochCounter` = 3000)
  - `stepsize`: #iterations to reach 1/2 cycle (1/2 epoch)
  - `opt.LR`: current LR
  - `cycle`: which cycle the model's currently in (float), always starts at 1 (first cycle)
  - `x`: which part of the cycle, in range \[0,1\]
  - `lr`: the updated lr, with `triangular` method.
## Conclusions
### The author's conclusions
- All experiments show similar or better accuracy performance when using CLR versus using a fixed learning rate, even though the performance drops at some of the learning rate values within this range.
### Rating
Noice
### My Conclusion
- A very nice and easy paper to read.
- CLR is an awesome technique to control the learning rate. This should be used when we started a new model or a new dataset, giving a very nice baseline for further optimization.
- CLR is also widely used by kagglers.
### Cited references and used images from:
- https://www.jeremyjordan.me/nn-learning-rate/ 
- https://github.com/bckenstler/CLR/
### Source code/other links
- TBE