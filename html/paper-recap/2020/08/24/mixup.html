<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Paper recap: MixUp: Beyond empirical risk minimization
  </title>
  <meta
    name="description"
    content="MixUp: Beyond empirical risk minimizationZhang, Hongyi Cisse, Moustapha Dauphin, Yann N.Lopez-Paz, David, 2018Research Topic  Category (General): Deep Learni..."
  />

   
  <link rel="stylesheet" href="/assets/style.css" />

  <link
    rel="canonical"
    href="https://trung-dt.com/paper-recap/2020/08/24/mixup.html"
  />
  <link rel="alternate" type="application/rss+xml" title="Trung Dao"
  href="https://trung-dt.com/feed.xml">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="stylesheet" href="/assets/academicons.min.css"/>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  

  <link rel="manifest" href="/site.webmanifest" />
  <script async defer src="/assets/github-buttons.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: [
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js",
        "a11y/accessibility-menu.js"
      ],
      jax: ["input/TeX", "output/CommonHTML"],
      TeX: {
        extensions: [
          "AMSmath.js",
          "AMSsymbols.js",
          "noErrors.js",
          "noUndefined.js",
        ]
      }
    });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      var TEX = MathJax.InputJax.TeX;
      var COLS = function (W) {
        var WW = [];
        for (var i = 0, m = W.length; i < m; i++)
          {WW[i] = TEX.Parse.prototype.Em(W[i])}
        return WW.join(" ");
      };
      TEX.Definitions.Add({
        environment: {
          psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
        }
      });
    });
  </script>

  <!-- Mathjax Support -->
  <script
    type="text/javascript"
    async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
  ></script>


 <script>
  $(document).ready(function() {
    console.log("hello"); // This will print "hello" to the console if the JavaScript is running

    $('.publication').mouseover(function() {
      console.log("Mouseover event triggered");
      const video = $(this).find('video')[0]; // Get the DOM element
      console.log("Video element found:", video);
        $(video).css('display', 'block');
        $(this).find('img').css('display', 'none');
    });

    $('.publication').mouseout(function() {
      console.log("Mouseout event triggered");
      const video = $(this).find('video')[0]; // Get the DOM element
      console.log("Video element found:", video);
        $(video).css('display', 'none');
        $(this).find('img').css('display', 'block');
    });
  });
</script> 
  <script src="https://kit.fontawesome.com/a24e8ec4a3.js"></script>
</head>


  <body>
    <header class="border-bottom-thick px-2 clearfix">
  <div id="particles-js" class="particles"></div>
<script src="https://trung-dt.com/assets/particles.js/particles.js"></script>
<script src="https://trung-dt.com/assets/particles.js/dir/js/app.js"></script>

  <div class="left sm-width-full py-1 mt-1 mt-lg-0">
    <a class="align-middle link-primary text-accent" href="/">
      Trung Dao
    </a>
  </div>
  <div class="right sm-width-full">
    <ul class="list-reset right py-1 header-text font-smoothing">
   
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/blog/"
      target="_blank"
      >üê¢ Blogs</a
    >
  </li>
    
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/cv/"
      target="_blank"
      >üéì CV</a
    >
  </li>
    
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/gallery/"
      target="_blank"
      >üíé Film Photos</a
    >
  </li>
         
</ul>

  </div>
</header>


    <div>
      <article
  class="container px-2 mx-auto mb4"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <h1
    id="post-title"
    class="h0 col-9 sm-width-full py-4 mt-3 inline-block"
    itemprop="name headline"
  >
    Paper recap: MixUp: Beyond empirical risk minimization
  </h1>
  <div class="col-4 sm-width-full mt-1 border-top-thin">
    <p class="mb-3 py-2 bold h4">
      <time
        datetime="2020-08-24T00:00:00+07:00"
        itemprop="datePublished"
        >Aug 24, 2020
      </time>
      |   8 mins 
    </p>
  </div>

  <div class="prose" itemprop="articleBody">
    <h2 id="mixup-beyond-empirical-risk-minimization">MixUp: Beyond empirical risk minimization</h2>
<p><a href="https://arxiv.org/abs/1710.09412">Zhang, Hongyi Cisse, Moustapha Dauphin, Yann N.Lopez-Paz, David, 2018</a></p>

<h2 id="research-topic">Research Topic</h2>
<ul>
  <li>Category (General): Deep Learning</li>
  <li>Category (Specific): Data Augmentation</li>
</ul>

<h2 id="paper-summary">Paper summary</h2>
<ul>
  <li>Introduce a new data augmentation method: MixUp, which helps:
    <ul>
      <li>Improve generalization of neural network architectures.</li>
      <li>Reduces memorization of corrupt labels.</li>
      <li>Increases the robustness to adversarial learning.</li>
      <li>Stabilize the training of GANs.</li>
    </ul>
  </li>
</ul>

<h2 id="explain-like-im-5-eli5-">Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂</h2>
<ul>
  <li>Well this gif sums up pretty well‚Ä¶
<img src="https://media.giphy.com/media/BHeCjdyGJck6c/giphy.gif" alt="pineapple-pen" /></li>
</ul>

<h2 id="issues-addressed-by-the-paper">Issues addressed by the paper</h2>
<ul>
  <li>As the model is being trained by minimizing the loss/objective function using a known set of training data (Empirical Risk Minimization), it can choose to <em>memorize</em> the training data rather than <em>generalize</em> it, even when in the presence of strong regularization. So the model will perform terribly when evaluating on examples outside the training distribution.</li>
  <li>Normal data augmentation can help model to improve generalization, but it requires costly computations, and also needs human experts to decide how to augment the data.</li>
</ul>

<h2 id="approachmethod">Approach/Method</h2>
<ul>
  <li>Core idea: Linear interpolate (lerp) two images and its labels together, then do it for the whole batch. Use soft label for better performance. How much to interpolate is used by a number call <code class="language-plaintext inlined highlighter-rouge">lambda</code> generated by Beta Distribution.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">new_image</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span> <span class="n">image_1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="k">lambda</span><span class="p">)</span> <span class="o">*</span> <span class="n">image_2</span>
  <span class="n">new_label</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span> <span class="n">label_1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="k">lambda</span><span class="p">)</span> <span class="o">*</span> <span class="n">label_2</span>
</code></pre></div>    </div>
  </li>
  <li><code class="language-plaintext inlined highlighter-rouge">lambda</code> is sampled from symmetric Beta distribution (having same \(\alpha\) and \(\beta\) value), which looks like the following image. This distribution makes sure that the new image will most of the time be close only to the first or the second picture. Only sometimes both images have the same intensity (of course if in a case of multicategorical, other label will be 0).</li>
</ul>

<p align="center">
  <img src="/assets/images/mixup/beta.png" style="width: 50%;" alt="lr" />
</p>

<ul>
  <li>They tried multiple ways to pick images:
    <ul>
      <li>Use 2 different dataloader, take one of each then interpolate -&gt; good mix, but need 2 loaders which takes more time to run.</li>
      <li>Use more than two pictures to mix: non-significant improvement, but increases the computation cost.</li>
      <li>Use only 1 dataloader, then mixup the batch and its shuffled-version -&gt; may have duplicates, but peforms equally well -&gt; <strong>RECOMMEND</strong>.</li>
    </ul>
  </li>
</ul>

<h2 id="result">Result</h2>
<ul>
  <li>Reduces the amount of undesirable oscillations when predicting outside the training examples, also makes memorization more difficult to achieve (generalizes better). The author had make tests by training both normal (ERM) and mixup model against randomly corruptled labels:
    <ul>
      <li>As you can see, the ERM model starts to overfit the corrupted labels when the LR starts to slow down to fine tune.</li>
      <li>mixup model doesn‚Äôt do the same, so when it‚Äôs tested against the real label, it still performs fairly well -&gt; not overfitted.
<img src="/assets/images/mixup/generalize.png" alt="generalize" /></li>
    </ul>
  </li>
  <li>Leads to decision boundaries that transition linearly from class to class, providing a smoother estimate of uncertainty:
    <ul>
      <li>In the ERM model, if an adversarial example (class orange) can push through the blue boundary, the model will definately determine that example is class 0.</li>
      <li>On the other hand, since mixup model produce a smooth boundary, it may still classify that example correctly.</li>
    </ul>
  </li>
</ul>
<p align="center">
  <img src="/assets/images/mixup/boundaries.png" style="width: 75%;" alt="lr" />
</p>
<ul>
  <li>Significantly improve the robustness of neural networks without hindering the speed of ERM.</li>
</ul>

<h2 id="best-practice">Best practice</h2>
<ul>
  <li>\(\alpha \in [0.1, 0.4]\) usually performs well, whereas larger \(\alpha\) may lead to underfitting.</li>
  <li>When using mixup, use larger neural network and also train longer for better result.</li>
  <li>mixup + dropout performs very nicely together (produced SOTA in the paper).</li>
</ul>

<h2 id="hidden-gems">Hidden gemsüíéüíéüíé</h2>
<ul>
  <li>For normal data augmentation, one usually uses rotation, translation, cropping, resizing, flipping and random erasing to enforce visually plausible invariances in the model through the training data.</li>
</ul>

<h2 id="conclusions">Conclusions</h2>

<h3 id="rating">Rating</h3>
<p><img src="https://media.giphy.com/media/8xfNjg3mJPj68/giphy.gif" alt="rating" /></p>

<h2 id="paper-implementation">Paper implementation</h2>
<ul>
  <li>Using Pytorch Lightning Callback system, inspired by fastai implementation.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">class</span> <span class="nc">MixLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old_lf</span><span class="p">,</span> <span class="n">mixup_cb</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span> <span class="o">=</span> <span class="n">old_lf</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span> <span class="o">=</span> <span class="n">mixup_cb</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
          <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">pl_module</span><span class="p">.</span><span class="n">testing</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
          <span class="k">with</span> <span class="n">NoneReduce</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span><span class="p">)</span> <span class="k">as</span> <span class="n">lf</span><span class="p">:</span>
              <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">yb_1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">yb_1</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
              <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">lam</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">lam</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
              <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">lf</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">yb_1</span><span class="p">),</span> <span class="n">lf</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">yb</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">mixup_cb</span><span class="p">.</span><span class="n">lam</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span><span class="p">,</span> <span class="s">'reduction'</span><span class="p">,</span> <span class="s">'mean'</span><span class="p">))</span>

  <span class="k">class</span> <span class="nc">MixupDict</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">distrib</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>

      <span class="k">def</span> <span class="nf">on_train_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span> <span class="o">=</span> <span class="n">pl_module</span><span class="p">.</span><span class="n">loss_func</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">loss_fnc</span> <span class="o">=</span> <span class="n">MixLoss</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
          <span class="n">pl_module</span><span class="p">.</span><span class="n">loss_func</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss_fnc</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">pl_module</span> <span class="o">=</span> <span class="n">pl_module</span>

      <span class="k">def</span> <span class="nf">_mixup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logger</span><span class="p">,</span> <span class="n">log_image</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">pre_fix</span><span class="o">=</span><span class="s">'train'</span><span class="p">):</span>
          <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">"img"</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s">"label"</span><span class="p">]</span>
          <span class="n">bs</span> <span class="o">=</span> <span class="n">yb</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

          <span class="c1"># Produce "bs" probability for each sample
</span>          <span class="n">lam</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">distrib</span><span class="p">.</span><span class="n">sample</span><span class="p">((</span><span class="n">bs</span><span class="p">,)).</span><span class="n">squeeze</span><span class="p">()</span>

          <span class="c1"># Get those probability that &gt;0.5, so that the first img (in the nonshuffle batch) has bigger coeff
</span>          <span class="c1"># Which avoid duplication mixup
</span>          <span class="n">lam</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">lam</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">lam</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
          <span class="bp">self</span><span class="p">.</span><span class="n">lam</span> <span class="o">=</span> <span class="n">lam</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

          <span class="c1"># Permute the batch
</span>          <span class="n">shuffle</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">bs</span><span class="p">)</span>
          <span class="n">xb_1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">yb_1</span> <span class="o">=</span> <span class="n">xb</span><span class="p">[</span><span class="n">shuffle</span><span class="p">],</span> <span class="n">yb</span><span class="p">[</span><span class="n">shuffle</span><span class="p">]</span>
          <span class="n">nx_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
          <span class="n">weight</span> <span class="o">=</span> <span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lam</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">nx_dims</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
          <span class="n">x_new</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">xb_1</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">log_image</span><span class="p">:</span>
              <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">x_new</span><span class="p">)</span>
              <span class="n">logger</span><span class="p">.</span><span class="n">experiment</span><span class="p">.</span><span class="n">add_image</span><span class="p">(</span><span class="n">pre_fix</span> <span class="o">+</span> <span class="s">'mixup'</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
              <span class="n">grid_g</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
              <span class="n">logger</span><span class="p">.</span><span class="n">experiment</span><span class="p">.</span><span class="n">add_image</span><span class="p">(</span><span class="n">pre_fix</span> <span class="o">+</span> <span class="s">'norm'</span><span class="p">,</span> <span class="n">grid_g</span><span class="p">)</span>
              <span class="n">dif</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">xb</span> <span class="o">-</span> <span class="n">x_new</span><span class="p">)</span>
              <span class="n">grid_d</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">dif</span><span class="p">)</span>
              <span class="n">logger</span><span class="p">.</span><span class="n">experiment</span><span class="p">.</span><span class="n">add_image</span><span class="p">(</span><span class="n">pre_fix</span> <span class="o">+</span> <span class="s">'dif'</span><span class="p">,</span> <span class="n">grid_d</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">x_new</span>

      <span class="k">def</span> <span class="nf">on_train_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_mixup</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">trainer</span><span class="p">.</span><span class="n">logger</span><span class="p">)</span>
          <span class="n">batch</span><span class="p">[</span><span class="s">"img"</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>

      <span class="k">def</span> <span class="nf">on_validation_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
          <span class="n">pl_module</span><span class="p">.</span><span class="n">loss_func</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span>

      <span class="k">def</span> <span class="nf">on_validation_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
          <span class="n">pl_module</span><span class="p">.</span><span class="n">loss_func</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">loss_fnc</span>

      <span class="k">def</span> <span class="nf">on_fit_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
          <span class="n">pl_module</span><span class="p">.</span><span class="n">loss_func</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_lf</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="papers-needs-to-conquer-next-">Papers needs to conquer next üëèüëèüëè</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1905.04899">Cutmix</a></li>
</ul>

  </div>
</article>

<div class="container mx-auto px-2 py-2 clearfix">
  <!-- Use if you want to show previous and next for posts within a category. -->




<div class="col-4 sm-width-full left mr-lg-4 mt-3">
  <a
    class="no-underline border-top-thin py-1 block"
    href="https://trung-dt.com/paper-recap/2020/08/17/sgdr.html"
  >
    <span class="h5 bold text-accent">Previous</span>
    <p class="bold h3 link-primary mb-1">Paper recap: SGDR, Super-convergence</p>
    <p>SGDR: Stochastic Gradient Descent with Warm Restarts Ilya Loshchilov, Frank Hutter, 2016 Super-Convergence: Very Fast Training of Neural Networks Using...</p>
  </a>
</div>
 
<div class="col-4 sm-width-full left mt-3">
  <a
    class="no-underline border-top-thin py-1 block"
    href="https://trung-dt.com/paper-recap/2020/08/31/cutmix.html"
  >
    <span class="h5 bold text-accent">Next</span>
    <p class="bold h3 link-primary mb-1">Paper recap: CutMix</p>
    <p>## CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features [Yun, Sangdoo Han, Dongyoon Chun, Sanghyuk Oh, Seong Joon...</p>
  </a>
</div>


</div>




    </div>

    <div class="border-top-thin clearfix mt-2 mt-lg-4">
  <div class="container mx-auto px-2">
    <p class="col-8 sm-width-full left py-2 mb-0">
      2024. All rights Reserved. This website doesn't track
      you. Thanks to
      <a class="text-accent" href="https://giphy.com/">GIPHY</a> for GIFs!
    </p>
  </div>
</div>

  </body>
</html>
