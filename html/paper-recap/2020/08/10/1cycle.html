<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Paper recap: A disciplined approach to neural network hyper-parameters: Part 1
  </title>
  <meta
    name="description"
    content="A disciplined approach to neural network hyper-parameters: Part 1Lesli N.Smith, 2018Research Topic  Category (General): Deep Learning.  Category (Specific): ..."
  />

   
  <link rel="stylesheet" href="/assets/style.css" />

  <link
    rel="canonical"
    href="https://trung-dt.com/paper-recap/2020/08/10/1cycle.html"
  />
  <link rel="alternate" type="application/rss+xml" title="Trung Dao"
  href="https://trung-dt.com/feed.xml">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="stylesheet" href="/assets/academicons.min.css"/>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  

  <link rel="manifest" href="/site.webmanifest" />
  <script async defer src="/assets/github-buttons.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: [
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js",
        "a11y/accessibility-menu.js"
      ],
      jax: ["input/TeX", "output/CommonHTML"],
      TeX: {
        extensions: [
          "AMSmath.js",
          "AMSsymbols.js",
          "noErrors.js",
          "noUndefined.js",
        ]
      }
    });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      var TEX = MathJax.InputJax.TeX;
      var COLS = function (W) {
        var WW = [];
        for (var i = 0, m = W.length; i < m; i++)
          {WW[i] = TEX.Parse.prototype.Em(W[i])}
        return WW.join(" ");
      };
      TEX.Definitions.Add({
        environment: {
          psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
        }
      });
    });
  </script>

  <!-- Mathjax Support -->
  <script
    type="text/javascript"
    async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
  ></script>


 <script>
  $(document).ready(function() {
    console.log("hello"); // This will print "hello" to the console if the JavaScript is running

    $('.publication').mouseover(function() {
      console.log("Mouseover event triggered");
      const video = $(this).find('video')[0]; // Get the DOM element
      console.log("Video element found:", video);
        $(video).css('display', 'block');
        $(this).find('img').css('display', 'none');
    });

    $('.publication').mouseout(function() {
      console.log("Mouseout event triggered");
      const video = $(this).find('video')[0]; // Get the DOM element
      console.log("Video element found:", video);
        $(video).css('display', 'none');
        $(this).find('img').css('display', 'block');
    });
  });
</script> 
  <script src="https://kit.fontawesome.com/a24e8ec4a3.js"></script>
</head>


  <body>
    <header class="border-bottom-thick px-2 clearfix">
  <div id="particles-js" class="particles"></div>
<script src="https://trung-dt.com/assets/particles.js/particles.js"></script>
<script src="https://trung-dt.com/assets/particles.js/dir/js/app.js"></script>

  <div class="left sm-width-full py-1 mt-1 mt-lg-0">
    <a class="align-middle link-primary text-accent" href="/">
      Trung Dao
    </a>
  </div>
  <div class="right sm-width-full">
    <ul class="list-reset right py-1 header-text font-smoothing">
   
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/blog/"
      target="_blank"
      >üê¢ Blogs</a
    >
  </li>
    
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/cv/"
      target="_blank"
      >üéì CV</a
    >
  </li>
    
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/gallery/"
      target="_blank"
      >üíé Film Photos</a
    >
  </li>
         
</ul>

  </div>
</header>


    <div>
      <article
  class="container px-2 mx-auto mb4"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <h1
    id="post-title"
    class="h0 col-9 sm-width-full py-4 mt-3 inline-block"
    itemprop="name headline"
  >
    Paper recap: A disciplined approach to neural network hyper-parameters: Part 1
  </h1>
  <div class="col-4 sm-width-full mt-1 border-top-thin">
    <p class="mb-3 py-2 bold h4">
      <time
        datetime="2020-08-10T00:00:00+07:00"
        itemprop="datePublished"
        >Aug 10, 2020
      </time>
      |   7 mins 
    </p>
  </div>

  <div class="prose" itemprop="articleBody">
    <h2 id="a-disciplined-approach-to-neural-network-hyper-parameters-part-1">A disciplined approach to neural network hyper-parameters: Part 1</h2>
<p><a href="http://arxiv.org/abs/1803.09820">Lesli N.Smith, 2018</a></p>

<h2 id="research-topic">Research Topic</h2>
<ul>
  <li>Category (General): Deep Learning.</li>
  <li>Category (Specific): Hyperparameters Tuning.</li>
</ul>

<h2 id="paper-summary">Paper summary</h2>
<ul>
  <li>Introduce techniques to set some essential hyper-parameters such as: learning rate, batch size, momentum, weight decay.</li>
  <li>How to examine training/ test loss curve for clues of underfitting, overfitting.</li>
  <li>Introduce a new method of cyclical learning rate: <strong>1cycle policy</strong>.</li>
  <li>Discuss about cyclical momentum, cyclical weight decay.</li>
  <li>Produce multiples examples to show the importance of balanced regularization for each dataset/ architecture.</li>
</ul>

<h2 id="explain-like-im-5-eli5-">Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂</h2>
<ul>
  <li>Not necessary since this paper mostly tackle best pratices while working on projects.</li>
</ul>

<h2 id="issues-addressed-by-the-paper">Issues addressed by the paper</h2>
<ul>
  <li>Grid-search is computationally expensive and time consumming. But a good choice of hyper-parameters is vital for a model to perform well, so how to do grid-search efficiently?</li>
  <li>Underfitting/ overfitting trade-off.</li>
</ul>

<ol>
  <li>Learning rate:
    <ul>
      <li>Too small: overfitting can occur.</li>
      <li>Too large: can have regularize but will diverge.</li>
    </ul>
  </li>
  <li>Batch size:
    <ul>
      <li>Suggest that when we comparing batch size, neither maintaining <em>constant #epochs</em> (train the same #epochs for each batch size) nor <em>constant #iterations</em> (train the same #iterations/epoch each batch size) is appropriate:
        <ul>
          <li>constant #epochs:
            <ul>
              <li>Computationally efficient, but penalized more since we see a big proportion of samples each time.</li>
            </ul>
          </li>
          <li>constant #iterations:
            <ul>
              <li>Overfit will occur.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Batch size directly affect computational time.</li>
    </ul>
  </li>
  <li>Momentum:
    <ul>
      <li>Momentum and learning rate are closely related and its optimal values are dependent on each other.</li>
      <li>Momentum‚Äôs effect on updating the weights is of the same magnitude as the learning rate (used SGD with momentum equation to verify).</li>
    </ul>
  </li>
  <li>Weight decay:
    <ul>
      <li>Weight decay is one form of regularization and it plays an important role in training so its value needs to be set properly.</li>
    </ul>
  </li>
</ol>

<h2 id="approachmethod">Approach/Method</h2>
<!-- ![lr](/assets/images/1cycle/lr.png) -->
<p align="center">
  <img src="/assets/images/1cycle/lr.png" style="width: 50%;" alt="lr" />
</p>

<ul>
  <li>1cycle policy:
    <ul>
      <li>Instead of cyclical learning rate using <code class="language-plaintext inlined highlighter-rouge">triangular</code> as the <a href="/paper-recap/2020/08/03/clr.html">previous post</a>, the author suggest to:
        <ol>
          <li>Train all epochs in more than one cycle just a small proportion.</li>
          <li>In the remaining iterations, the learning rate will decline from <code class="language-plaintext inlined highlighter-rouge">base_lr</code> to several orders of magnitude less.</li>
        </ol>
      </li>
      <li>Experiments show that this policy allows the accuracy to plateau before the training ends.</li>
    </ul>
  </li>
  <li>The author show these 6 remarks after multiple researchs and experiments:
    <ol>
      <li>The test/validation loss is a good indicator of the network‚Äôs convergence and should be examined for clues.
        <ul>
          <li>Look at the loss curve and also plot generalization error curve (<code class="language-plaintext inlined highlighter-rouge">valid_loss - train_loss</code>), one can determine whether the architechture has the capacity to overfit or has too small learning rate (which also leads to overfit)</li>
        </ul>
      </li>
      <li>Achieving the horizontal part of the test loss is the goal of hyperparameter tuning.
  <img src="/assets/images/1cycle/under_over_plot.png" alt="under_over_plot" />
        <ul>
          <li>The horizontal part is the red line.</li>
        </ul>
      </li>
      <li>The amount of regularization must be balanced for each dataset and architecture.</li>
      <li>The practitioner‚Äôs goal is obtaining the highest performance while minimizing the needed computational time.</li>
      <li>Optimal momentum value(s) will improve network training.</li>
      <li>Since the amount of regularization must be balanced for each dataset and architecture, the value of weight decay is a key knob to turn for tuning regularization against the regularization from an increasing learning rate.</li>
    </ol>
  </li>
</ul>

<h2 id="best-practices">Best practices</h2>
<p><img src="/assets/images/1cycle/lr_m.png" alt="lr_m" /></p>
<ol>
  <li>Learning rate:
    <ul>
      <li>Use learning rate range test to find the minimum and maximum learning rate boundaries:
        <ul>
          <li>Maximum learning rate bound: the maximum value that the model can still converge</li>
          <li>Minumum learning rate bound:
            <ol>
              <li>\(\frac{1}{3};\frac{1}{4}\) of max bound.</li>
              <li>\(\frac{1}{10};\frac{1}{20}\) of max bound if using 1cycle.</li>
            </ol>
          </li>
        </ul>
      </li>
      <li>Use 1cycle policy to achive super-convergence (reachs global optima with iterations much less than regulars).</li>
      <li>Other regularization methods must be reduced to compensate for the regularization effects of large learning rates.</li>
    </ul>
  </li>
  <li>Batch size:
    <ul>
      <li>Small batch sizes add regularization, large batch sizes add less; utilize this while balancing the proper amount of regularization.</li>
      <li>Often better to use a larger batch size so a larger learning rate can be used (leads to using a larger batch size when using the 1cycle learning rate schedule).</li>
    </ul>
  </li>
  <li>Momentum:
    <ul>
      <li>Short runs with momentum values of 0.99, 0.97, 0.95, and 0.9 will quickly show the best value for momentum.</li>
      <li>If use 1cycle policy, should use cyclical momentum starting at maximum momentum value and decreasing to a value of 0.8 or 0.85 (performance is almost independent of the minimum momentum value).</li>
      <li>Decreasing cyclical momentum when the learning rate increases provides an equivalent result to the best constant momentum value.</li>
      <li>Using cyclical momentum along with the LR range test stabilizes the convergence when using large learning rate values more than a constant momentum does.</li>
    </ul>
  </li>
  <li>Weight decay:
    <ul>
      <li>Should be a constant value.</li>
      <li>Should use grid search to find a proper value; validation loss early in the training is sufficient for determining a good value.</li>
      <li>Another option as a grid search for weight decay is to make a single run at a middle value for weight decay and save a snapshot after the loss plateaus. Use this snapshot to restart runs, each with a different value of WD. This can save time in searching for the best weight decay.</li>
      <li>A complex dataset requires less regularization so test smaller weight decay values, such as \(10^{‚àí4}, 10^{‚àí5}, 10^{‚àí6}, 0\)</li>
      <li>A shallow architecture requires more regularization so test larger weight decay values, such as \(10^{‚àí2}, 10^{‚àí3}, 10^{‚àí4}\).</li>
      <li>The optimal weight decay is different if you search with a constant learning rate versus using a learning rate range.</li>
    </ul>
  </li>
</ol>

<h2 id="hidden-gems">Hidden gemsüíéüíéüíé</h2>
<p><img src="/assets/images/1cycle/test_loss_dec.png" alt="test_loss_dec" /></p>

<ul>
  <li>Test loss decreases more rapidly during the initial iterations and is then horizontal is an early positive clue indicating that the model will produce a better final accuracy. (Blue curve)</li>
  <li>Learning rates that are too small can exhibit some overfitting behavior.</li>
  <li>There is a maximum speed the learning rate can increase without the training becoming unstable.</li>
  <li>The very large learning rates provided the twin benefits of regularization that prevented overfitting and faster training of the network.</li>
  <li>Set momentum as large as possible without causing instabilities during training.</li>
  <li>Momentum range test is not useful for finding an optimal momentum, you should use grid search.</li>
  <li>Decreasing the momentum while the learning rate increases provides three benefits (by experiments):
    <ol>
      <li>a lower minimum test loss.</li>
      <li>faster initial convergence.</li>
      <li>greater convergence stability over a larger range of learning rates.</li>
    </ol>
  </li>
  <li>Large momentum helps escape saddle points but can hurt the final convergence, implying that momentum should be reduced at the end of training.</li>
  <li>A good procedure is to test momentum values in the range of 0.9 to 0.99.</li>
  <li>All the general ideas can apply to shallow or deep networks, although the details (i.e., specific values for momentum) varied.</li>
</ul>

<h2 id="results">Results</h2>
<p><img src="/assets/images/1cycle/result.png" alt="result" /></p>
<ul>
  <li>As the experiment shown, if one can find optimal values for these hyper-parameters, the model would achieve super-convergence, which saves computational cost and time.
    <h2 id="limitations">Limitations</h2>
  </li>
  <li>These disciplines are not proved but mostly achieved by experiments. So we can only use this as a guide and apply it to our projects.</li>
</ul>

<h2 id="confusing-aspects-of-the-paper">Confusing aspects of the paper</h2>
<ul>
  <li>Very straightforward, confusing-free.</li>
</ul>

<h2 id="conclusions">Conclusions</h2>

<h3 id="rating">Rating</h3>
<p>Would read again.</p>

<h3 id="my-conclusion">My conclusion</h3>
<ul>
  <li>Good workflow to deal with hyper-parameters.</li>
  <li>Can be use as a reference when start a new projects.</li>
</ul>

<h2 id="paper-implementation">Paper implementation</h2>
<p><img src="https://media.giphy.com/media/nKZEvTua5D4o0XD6Ge/giphy.gif" alt="Nope" /></p>

<h2 id="cited-references-and-used-images-from">Cited references and used images from:</h2>
<ul>
  <li><a href="http://arxiv.org/abs/1803.09820">Lesli N.Smith, 2018</a></li>
  <li>https://sgugger.github.io/the-1cycle-policy.html</li>
  <li>https://github.com/asvcode/1_cycle</li>
</ul>

<h2 id="papers-needs-to-conquer-next-">Papers needs to conquer next üëèüëèüëè</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1608.03983">SGDR</a></li>
</ul>

  </div>
</article>

<div class="container mx-auto px-2 py-2 clearfix">
  <!-- Use if you want to show previous and next for posts within a category. -->




<div class="col-4 sm-width-full left mr-lg-4 mt-3">
  <a
    class="no-underline border-top-thin py-1 block"
    href="https://trung-dt.com/paper-recap/2020/08/03/clr.html"
  >
    <span class="h5 bold text-accent">Previous</span>
    <p class="bold h3 link-primary mb-1">Paper recap: Cyclical Learning Rates for Training Neural Networks</p>
    <p>Cyclical Learning Rates for Training Neural Networks Lesli N.Smith, 2017 Research Topic Category (General): Deep Learning Category (Specific): Hyperparameters (Learning...</p>
  </a>
</div>
 
<div class="col-4 sm-width-full left mt-3">
  <a
    class="no-underline border-top-thin py-1 block"
    href="https://trung-dt.com/paper-recap/2020/08/17/sgdr.html"
  >
    <span class="h5 bold text-accent">Next</span>
    <p class="bold h3 link-primary mb-1">Paper recap: SGDR, Super-convergence</p>
    <p>## SGDR: Stochastic Gradient Descent with Warm Restarts [Ilya Loshchilov, Frank Hutter, 2016](https://arxiv.org/abs/1506.01186) ## Super-Convergence: Very Fast Training of Neural...</p>
  </a>
</div>


</div>




    </div>

    <div class="border-top-thin clearfix mt-2 mt-lg-4">
  <div class="container mx-auto px-2">
    <p class="col-8 sm-width-full left py-2 mb-0">
      2024. All rights Reserved. This website doesn't track
      you. Thanks to
      <a class="text-accent" href="https://giphy.com/">GIPHY</a> for GIFs!
    </p>
  </div>
</div>

  </body>
</html>
