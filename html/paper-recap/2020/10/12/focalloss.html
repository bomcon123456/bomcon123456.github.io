<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    Paper recap: Focal Loss for Dense Object Detection
  </title>
  <meta
    name="description"
    content="Focal Loss for Dense Object DetectionLin et al, 2020"
  />

   
  <link rel="stylesheet" href="/assets/style.css" />

  <link
    rel="canonical"
    href="https://trung-dt.com/paper-recap/2020/10/12/focalloss.html"
  />
  <link rel="alternate" type="application/rss+xml" title="Trung Dao"
  href="https://trung-dt.com/feed.xml">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="stylesheet" href="/assets/academicons.min.css"/>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  

  <link rel="manifest" href="/site.webmanifest" />
  <script async defer src="/assets/github-buttons.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: [
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js",
        "a11y/accessibility-menu.js"
      ],
      jax: ["input/TeX", "output/CommonHTML"],
      TeX: {
        extensions: [
          "AMSmath.js",
          "AMSsymbols.js",
          "noErrors.js",
          "noUndefined.js",
        ]
      }
    });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      var TEX = MathJax.InputJax.TeX;
      var COLS = function (W) {
        var WW = [];
        for (var i = 0, m = W.length; i < m; i++)
          {WW[i] = TEX.Parse.prototype.Em(W[i])}
        return WW.join(" ");
      };
      TEX.Definitions.Add({
        environment: {
          psmallmatrix: ['Array',null,'(',')','c',COLS([1/3]),".2em",'S',1],
        }
      });
    });
  </script>

  <!-- Mathjax Support -->
  <script
    type="text/javascript"
    async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
  ></script>


 <script>
  $(document).ready(function() {
    console.log("hello"); // This will print "hello" to the console if the JavaScript is running

    $('.publication').mouseover(function() {
      console.log("Mouseover event triggered");
      const video = $(this).find('video')[0]; // Get the DOM element
      console.log("Video element found:", video);
        $(video).css('display', 'block');
        $(this).find('img').css('display', 'none');
    });

    $('.publication').mouseout(function() {
      console.log("Mouseout event triggered");
      const video = $(this).find('video')[0]; // Get the DOM element
      console.log("Video element found:", video);
        $(video).css('display', 'none');
        $(this).find('img').css('display', 'block');
    });
  });
</script> 
  <script src="https://kit.fontawesome.com/a24e8ec4a3.js"></script>
</head>


  <body>
    <header class="border-bottom-thick px-2 clearfix">
  <div id="particles-js" class="particles"></div>
<script src="https://trung-dt.com/assets/particles.js/particles.js"></script>
<script src="https://trung-dt.com/assets/particles.js/dir/js/app.js"></script>

  <div class="left sm-width-full py-1 mt-1 mt-lg-0">
    <a class="align-middle link-primary text-accent" href="/">
      Trung Dao
    </a>
  </div>
  <div class="right sm-width-full">
    <ul class="list-reset right py-1 header-text font-smoothing">
   
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/blog/"
      target="_blank"
      >üê¢ Blogs</a
    >
  </li>
    
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/cv/"
      target="_blank"
      >üéì CV</a
    >
  </li>
    
  <li class="inline-block">
    <a
      class="align-middle link-primary header-link mr-2"
      href="/gallery/"
      target="_blank"
      >üíé Film Photos</a
    >
  </li>
         
</ul>

  </div>
</header>


    <div>
      <article
  class="container px-2 mx-auto mb4"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <h1
    id="post-title"
    class="h0 col-9 sm-width-full py-4 mt-3 inline-block"
    itemprop="name headline"
  >
    Paper recap: Focal Loss for Dense Object Detection
  </h1>
  <div class="col-4 sm-width-full mt-1 border-top-thin">
    <p class="mb-3 py-2 bold h4">
      <time
        datetime="2020-10-12T00:00:00+07:00"
        itemprop="datePublished"
        >Oct 12, 2020
      </time>
      |   7 mins 
    </p>
  </div>

  <div class="prose" itemprop="articleBody">
    <h2 id="focal-loss-for-dense-object-detection">Focal Loss for Dense Object Detection</h2>
<p><a href="https://arxiv.org/abs/1708.02002">Lin et al, 2020</a></p>

<h2 id="research-topic">Research Topic</h2>
<ul>
  <li>Category (General): Deep Learning</li>
  <li>Category (Specific): Computer Vision</li>
</ul>

<h2 id="paper-summary">Paper summary</h2>
<ul>
  <li>Propose a new loss function tackling the extreme class imbalance problem encountered in object detections.</li>
  <li>Reshaping Cross Entropy Loss Function such that it down-weights the loss assigned to well-classified examples.</li>
  <li>Understand if one-stage de-tectors can match or surpass the accuracy of two-stage detectors while running at similar or faster speeds.</li>
  <li>Introduce the RetinaNet to evaluate the effectiveness of the loss, but since the author stated: ‚ÄúOur simple detector achieves top results not based on innovations in network design but due to our novel loss.‚Äù, I‚Äôm gonna skip the RetinaNet in this recap.</li>
</ul>

<h2 id="explain-like-im-5-eli5-">Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂</h2>
<ul>
  <li>Basically it is Balanced Cross Entropy Loss Function but with a tunable modulating factor such that it controls the weight loss of easy/hard samples.</li>
</ul>

<h2 id="issues-addressed-by-the-paper">Issues addressed by the paper</h2>
<ul>
  <li>Current state-of-the-art object detectors are based on a two-stage, proposal-driven mechanism, which  consistently achieves top accuracy on the challenging COCO benchmark.</li>
  <li>Could a simple one-stage detector achieve similar accuracy?</li>
  <li>Recent work on one-stage detectors yield faster detectors with accuracy within 10-40% relative to state-of-the-art two-stage methods, but takes also a lot of time.</li>
  <li>Class imbalance is addressed in R-CNN-like (one-stage) detectors by a two-stage cascade and sampling heuristics.</li>
  <li>An one-stage detector must process a much larger set of candidate object locations regularly sampled across an image, which makes the model inefficient as the training procedure is still dominated by easily classified background examples.</li>
  <li>Balanced CE loss function: while \(\alpha\) balances the importance of positive/negative examples, it does not differentiate between easy/hard examples.</li>
</ul>

<h2 id="approachmethod">Approach/Method</h2>
<ul>
  <li>Present a one-stage object detector (RetinaNet) that, for the first time (by the time of publish), matches the state-of-the-art COCO AP.</li>
  <li>To achieve this result:
    <ul>
      <li>Identify class imbalance during training as the main obstacle impeding one-stage detector from achieving state-of-the-art accuracy,</li>
      <li>Propose a new loss function that eliminates this barrier.</li>
    </ul>
  </li>
  <li>Focal Loss:
    <ul>
      <li>Dynamically scaled cross entropy loss.</li>
      <li>Automatically down-weight the contribution of easy examples (inliers) during training and rapidly focus the model on hard examples.</li>
      <li>Focal loss performs the <em>opposite role</em> of a robust loss (e.g: Huber Loss): it focuses training on a sparse set of hard examples.</li>
      <li>They first defined the probability function like this:</li>
    </ul>

\[p_t = \left\{
          \begin{array}{ll}
              p &amp; if \ y=1 \\
              1-p &amp; otherwise.
          \end{array}
      \right.\]

    <ul>
      <li>Using this definition, we can define CE loss func like this: \(CE(p,y) = CE(p_t) = -log(p_t)\) for comparision.</li>
      <li>Balanced Focal Loss‚Äôs definition:</li>
    </ul>

\[FL(p_t) = -\alpha_{t}(1-p_t)^{\gamma}log(p_t)\]

    <ul>
      <li>When an example is misclassified and \(p_t\) is small, the modulating factor is near 1 and the loss is unaffected. As \(p_t \to 1\), the factor goes to 0 and the loss for well-classified is down-weighted.</li>
      <li>The focusing parameter \(\gamma\) smoothly adjusts the rate at which easy examples are down-weighted.</li>
      <li>When \(\gamma = 0\), focal loss is equivalent to cross entropy loss, as \(\gamma\) is increased the effect of the modulating factor is likewise increased.</li>
    </ul>

    <p align="center">
      <img src="/assets/images/focal/focal_ce.png" style="width: 50%;" alt="focal_ce" />
  </p>
  </li>
</ul>

<h2 id="best-practice">Best practice</h2>
<ul>
  <li>\(\gamma = 2\) works well in practice.</li>
  <li>The benefit of changing \(\gamma\) is much larger, and indeed the best \(\alpha\)‚Äôs ranged in just [.25,.75].</li>
  <li>In general \(\alpha\) should be decreased slightly as \(\gamma\) is increased.</li>
</ul>

<h2 id="hidden-gems">Hidden gemsüíéüíéüíé</h2>
<ul>
  <li>In practice \(\alpha\) (of Balanced CE) may be set by inverse class frequency or treated as a hyperparameter to set by cross validation.</li>
  <li>One notable property of CE loss, which can be easily seen in the upper plot, is that even examples that are easily classified incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class.</li>
  <li>Two-stage detectors are often trained with the CE loss without use of \(\alpha\)-balancing, but address class imbalance through two mechanisms: (1) a two-stage cascade and (2) biased minibatch sampling.</li>
  <li>Feature Pyramid Network augments a standard convolutional network with a top-down pathway and lateral connections so the network efficiently constructs a rich, multi-scale feature pyramid from a single resolution input image.</li>
  <li>Larger backbone networks yield higher accuracy, but also slower inference speeds, likewise for input image scale.</li>
</ul>

<h2 id="confusing-aspects-of-the-paper">Confusing aspects of the paper</h2>
<ul>
  <li>The different between easy and hard sample?
    <ul>
      <li>The difference between the two is not obvious from the paper.</li>
      <li>IMO, easy samples means that the probability \(p_t\) is close to the class (1 for positive, 0 for negative), vice versa for the hard samples.</li>
    </ul>
  </li>
  <li>Why \(\alpha\)-balancing of CE doesn‚Äôt differentiate between easy/ hard examples?
    <ul>
      <li>This issue is also not verified in the paper.</li>
      <li>IMO, \(\alpha\) is only used to address the imbalanced class problem (since its value also tuned for the same reason), Focal Loss adds the modular factor which directly affected by the probability that the model suggested and adjust the loss.</li>
    </ul>
  </li>
</ul>

<h2 id="results">Results</h2>
<p align="center">
    <img src="/assets/images/focal/result.png" style="width: 80%;" alt="result" />
</p>
<ul>
  <li>The model achieves top results, outperforming both one-stage and two-stage models.</li>
</ul>

<h2 id="conclusions">Conclusions</h2>

<h3 id="the-authors-conclusions">The author‚Äôs conclusions</h3>
<ul>
  <li>Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples.</li>
  <li>The approach is simple and highly effective.</li>
</ul>

<h3 id="rating">Rating</h3>
<p><img src="https://media.giphy.com/media/3o6vY7UsuMPx3Yj9Xa/giphy.gif" alt="rating" /></p>

<h2 id="paper-implementation">Paper implementation</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FocalLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FocalLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_class</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">Variable</span><span class="p">)</span> <span class="k">else</span> <span class="n">Variable</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_class</span> <span class="o">=</span> <span class="n">n_class</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">device</span>
        <span class="n">bs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">n_channel</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">class_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">new</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">n_channel</span><span class="p">).</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">class_mask</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">class_mask</span><span class="p">)</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># onehot encoding
</span>        <span class="n">class_mask</span><span class="p">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ys</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs</span><span class="p">.</span><span class="n">is_cuda</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">ys</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="p">(</span><span class="n">probs</span><span class="o">*</span><span class="n">class_mask</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="n">log</span><span class="p">()</span>
        <span class="n">batch_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">alphas</span><span class="o">*</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">probs</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="p">))</span><span class="o">*</span><span class="n">log_p</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s">"mean"</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">batch_loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">batch_loss</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<h2 id="cited-references-and-used-images-from">Cited references and used images from:</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1708.02002">Focal Loss for Dense Object Detection</a></li>
  <li><a href="https://www.reddit.com/r/computervision/comments/9blsrl/focal_loss_for_dense_object_detection_retinanet/">Reddit post</a></li>
</ul>

<h2 id="papers-needed-to-be-conquered-next-">Papers needed to be conquered next üëèüëèüëè</h2>
<ul>
  <li>SIMCLR maybe?</li>
</ul>

  </div>
</article>

<div class="container mx-auto px-2 py-2 clearfix">
  <!-- Use if you want to show previous and next for posts within a category. -->




<div class="col-4 sm-width-full left mr-lg-4 mt-3">
  <a
    class="no-underline border-top-thin py-1 block"
    href="https://trung-dt.com/paper-recap/2020/09/07/bagoftricks.html"
  >
    <span class="h5 bold text-accent">Previous</span>
    <p class="bold h3 link-primary mb-1">Paper recap: Bag of Tricks for Image Classification with Convolutional Neural Networks</p>
    <p>Bag of Tricks for Image Classification with Convolutional Neural Networks He, Tong Zhang, Zhi Zhang, Hang Zhang, Zhongyue Xie, Junyuan...</p>
  </a>
</div>
 

</div>




    </div>

    <div class="border-top-thin clearfix mt-2 mt-lg-4">
  <div class="container mx-auto px-2">
    <p class="col-8 sm-width-full left py-2 mb-0">
      2024. All rights Reserved. This website doesn't track
      you. Thanks to
      <a class="text-accent" href="https://giphy.com/">GIPHY</a> for GIFs!
    </p>
  </div>
</div>

  </body>
</html>
