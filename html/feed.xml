<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://trung-dt.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://trung-dt.com/" rel="alternate" type="text/html" /><updated>2024-11-21T18:25:55+07:00</updated><id>https://trung-dt.com/feed.xml</id><title type="html">Trung Dao</title><subtitle>Greetings! I am a Research Resident at VinAI Research, where I am fortunate to work under the mentorship of Dr. Anh Tran and Dr. Cuong Pham. My primary research interests focus on Deep Generative Models, including GANs and Diffusion Models.

Previously, I worked as an AI Engineer also at VinAI, gaining hands-on experience in training, deploying, and optimizing deep learning models. During this period, I contributed to projects such as Face Recognition, Traffic Sign and Light Recognition, and Noise Cancelling, mostly deployed on edge devices, bridging the gap between research and real-world applications.

</subtitle><entry><title type="html">Paper recap: Focal Loss for Dense Object Detection</title><link href="https://trung-dt.com/paper-recap/2020/10/12/focalloss.html" rel="alternate" type="text/html" title="Paper recap: Focal Loss for Dense Object Detection" /><published>2020-10-12T00:00:00+07:00</published><updated>2020-10-12T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/10/12/focalloss</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/10/12/focalloss.html">&lt;h2 id=&quot;focal-loss-for-dense-object-detection&quot;&gt;Focal Loss for Dense Object Detection&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;Lin et al, 2020&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning&lt;/li&gt;
  &lt;li&gt;Category (Specific): Computer Vision&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Propose a new loss function tackling the extreme class imbalance problem encountered in object detections.&lt;/li&gt;
  &lt;li&gt;Reshaping Cross Entropy Loss Function such that it down-weights the loss assigned to well-classified examples.&lt;/li&gt;
  &lt;li&gt;Understand if one-stage de-tectors can match or surpass the accuracy of two-stage detectors while running at similar or faster speeds.&lt;/li&gt;
  &lt;li&gt;Introduce the RetinaNet to evaluate the effectiveness of the loss, but since the author stated: ‚ÄúOur simple detector achieves top results not based on innovations in network design but due to our novel loss.‚Äù, I‚Äôm gonna skip the RetinaNet in this recap.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-like-im-5-eli5-&quot;&gt;Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Basically it is Balanced Cross Entropy Loss Function but with a tunable modulating factor such that it controls the weight loss of easy/hard samples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Current state-of-the-art object detectors are based on a two-stage, proposal-driven mechanism, which  consistently achieves top accuracy on the challenging COCO benchmark.&lt;/li&gt;
  &lt;li&gt;Could a simple one-stage detector achieve similar accuracy?&lt;/li&gt;
  &lt;li&gt;Recent work on one-stage detectors yield faster detectors with accuracy within 10-40% relative to state-of-the-art two-stage methods, but takes also a lot of time.&lt;/li&gt;
  &lt;li&gt;Class imbalance is addressed in R-CNN-like (one-stage) detectors by a two-stage cascade and sampling heuristics.&lt;/li&gt;
  &lt;li&gt;An one-stage detector must process a much larger set of candidate object locations regularly sampled across an image, which makes the model inefficient as the training procedure is still dominated by easily classified background examples.&lt;/li&gt;
  &lt;li&gt;Balanced CE loss function: while \(\alpha\) balances the importance of positive/negative examples, it does not differentiate between easy/hard examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Present a one-stage object detector (RetinaNet) that, for the first time (by the time of publish), matches the state-of-the-art COCO AP.&lt;/li&gt;
  &lt;li&gt;To achieve this result:
    &lt;ul&gt;
      &lt;li&gt;Identify class imbalance during training as the main obstacle impeding one-stage detector from achieving state-of-the-art accuracy,&lt;/li&gt;
      &lt;li&gt;Propose a new loss function that eliminates this barrier.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Focal Loss:
    &lt;ul&gt;
      &lt;li&gt;Dynamically scaled cross entropy loss.&lt;/li&gt;
      &lt;li&gt;Automatically down-weight the contribution of easy examples (inliers) during training and rapidly focus the model on hard examples.&lt;/li&gt;
      &lt;li&gt;Focal loss performs the &lt;em&gt;opposite role&lt;/em&gt; of a robust loss (e.g: Huber Loss): it focuses training on a sparse set of hard examples.&lt;/li&gt;
      &lt;li&gt;They first defined the probability function like this:&lt;/li&gt;
    &lt;/ul&gt;

\[p_t = \left\{
          \begin{array}{ll}
              p &amp;amp; if \ y=1 \\
              1-p &amp;amp; otherwise.
          \end{array}
      \right.\]

    &lt;ul&gt;
      &lt;li&gt;Using this definition, we can define CE loss func like this: \(CE(p,y) = CE(p_t) = -log(p_t)\) for comparision.&lt;/li&gt;
      &lt;li&gt;Balanced Focal Loss‚Äôs definition:&lt;/li&gt;
    &lt;/ul&gt;

\[FL(p_t) = -\alpha_{t}(1-p_t)^{\gamma}log(p_t)\]

    &lt;ul&gt;
      &lt;li&gt;When an example is misclassified and \(p_t\) is small, the modulating factor is near 1 and the loss is unaffected. As \(p_t \to 1\), the factor goes to 0 and the loss for well-classified is down-weighted.&lt;/li&gt;
      &lt;li&gt;The focusing parameter \(\gamma\) smoothly adjusts the rate at which easy examples are down-weighted.&lt;/li&gt;
      &lt;li&gt;When \(\gamma = 0\), focal loss is equivalent to cross entropy loss, as \(\gamma\) is increased the effect of the modulating factor is likewise increased.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p align=&quot;center&quot;&gt;
      &lt;img src=&quot;/assets/images/focal/focal_ce.png&quot; style=&quot;width: 50%;&quot; alt=&quot;focal_ce&quot; /&gt;
  &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;best-practice&quot;&gt;Best practice&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;\(\gamma = 2\) works well in practice.&lt;/li&gt;
  &lt;li&gt;The benefit of changing \(\gamma\) is much larger, and indeed the best \(\alpha\)‚Äôs ranged in just [.25,.75].&lt;/li&gt;
  &lt;li&gt;In general \(\alpha\) should be decreased slightly as \(\gamma\) is increased.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hidden-gems&quot;&gt;Hidden gemsüíéüíéüíé&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In practice \(\alpha\) (of Balanced CE) may be set by inverse class frequency or treated as a hyperparameter to set by cross validation.&lt;/li&gt;
  &lt;li&gt;One notable property of CE loss, which can be easily seen in the upper plot, is that even examples that are easily classified incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class.&lt;/li&gt;
  &lt;li&gt;Two-stage detectors are often trained with the CE loss without use of \(\alpha\)-balancing, but address class imbalance through two mechanisms: (1) a two-stage cascade and (2) biased minibatch sampling.&lt;/li&gt;
  &lt;li&gt;Feature Pyramid Network augments a standard convolutional network with a top-down pathway and lateral connections so the network efficiently constructs a rich, multi-scale feature pyramid from a single resolution input image.&lt;/li&gt;
  &lt;li&gt;Larger backbone networks yield higher accuracy, but also slower inference speeds, likewise for input image scale.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;confusing-aspects-of-the-paper&quot;&gt;Confusing aspects of the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The different between easy and hard sample?
    &lt;ul&gt;
      &lt;li&gt;The difference between the two is not obvious from the paper.&lt;/li&gt;
      &lt;li&gt;IMO, easy samples means that the probability \(p_t\) is close to the class (1 for positive, 0 for negative), vice versa for the hard samples.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why \(\alpha\)-balancing of CE doesn‚Äôt differentiate between easy/ hard examples?
    &lt;ul&gt;
      &lt;li&gt;This issue is also not verified in the paper.&lt;/li&gt;
      &lt;li&gt;IMO, \(\alpha\) is only used to address the imbalanced class problem (since its value also tuned for the same reason), Focal Loss adds the modular factor which directly affected by the probability that the model suggested and adjust the loss.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/assets/images/focal/result.png&quot; style=&quot;width: 80%;&quot; alt=&quot;result&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The model achieves top results, outperforming both one-stage and two-stage models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;the-authors-conclusions&quot;&gt;The author‚Äôs conclusions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples.&lt;/li&gt;
  &lt;li&gt;The approach is simple and highly effective.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/3o6vY7UsuMPx3Yj9Xa/giphy.gif&quot; alt=&quot;rating&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;paper-implementation&quot;&gt;Paper implementation&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FocalLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;mean&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FocalLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_class&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_channel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;class_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_channel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;class_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# onehot encoding
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;class_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_cuda&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;log_p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;batch_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alphas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_p&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;mean&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cited-references-and-used-images-from&quot;&gt;Cited references and used images from:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;Focal Loss for Dense Object Detection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.reddit.com/r/computervision/comments/9blsrl/focal_loss_for_dense_object_detection_retinanet/&quot;&gt;Reddit post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;papers-needed-to-be-conquered-next-&quot;&gt;Papers needed to be conquered next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;SIMCLR maybe?&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">Focal Loss for Dense Object Detection Lin et al, 2020</summary></entry><entry><title type="html">Paper recap: Bag of Tricks for Image Classification with Convolutional Neural Networks</title><link href="https://trung-dt.com/paper-recap/2020/09/07/bagoftricks.html" rel="alternate" type="text/html" title="Paper recap: Bag of Tricks for Image Classification with Convolutional Neural Networks" /><published>2020-09-07T00:00:00+07:00</published><updated>2020-09-07T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/09/07/bagoftricks</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/09/07/bagoftricks.html">&lt;h2 id=&quot;bag-of-tricks-for-image-classification-with-convolutional-neural-networks&quot;&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.01187&quot;&gt;He, Tong Zhang, Zhi Zhang, Hang Zhang, Zhongyue Xie, Junyuan Li, Mu, 2019&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning&lt;/li&gt;
  &lt;li&gt;Category (Specific): Computer Vision&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Synthesize multiple effective methods that are briefly mentioned in litterature, in order to evaluate their impact on the final model accuracy through ablation study.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Find a combination of training procedure and model architecture refinements that improve model accuracy but barely change computational complexity.&lt;/li&gt;
  &lt;li&gt;These ‚Äútricks‚Äù usually are minor ones (such as changing the stride of a convolutional layer), but create a big impact.&lt;/li&gt;
  &lt;li&gt;These methods lead to significant accuracy improvement and combining them together can further boost the model accuracy. Transfer learning‚Äôs performance of these ‚Äúboosted‚Äù model would be much better too.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;

&lt;h3 id=&quot;training-baseline&quot;&gt;Training baseline&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Use the same training transformation (random sampling, random crop, random flip, hue/saturation/brightness, PCA noise, normalize).&lt;/li&gt;
  &lt;li&gt;Only resize in validation set.&lt;/li&gt;
  &lt;li&gt;Xavier initialization for weights of both convolutional and fully connected.&lt;/li&gt;
  &lt;li&gt;Use NAG (Nesterov), 120 epochs, 8 GPUs, batch-size = 256.&lt;/li&gt;
  &lt;li&gt;LR initialized to 0.1 and divided by 10 at the 30th, 60th, and 90th epochs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/baseline.png&quot; style=&quot;width: 50%;&quot; alt=&quot;baseline&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;efficient-training&quot;&gt;Efficient Training&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Larger training batch-size:
    &lt;ul&gt;
      &lt;li&gt;Since we‚Äôre using mini-batch SGD, for the same number of epochs, training with a large batch size results in a model with degraded validation accuracy compared to the ones trained with smaller batch sizes.&lt;/li&gt;
      &lt;li&gt;Some heuristics the help scale the batch size up like:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Linear scaling learning rate&lt;/strong&gt;: a large batch size reduces the noise in the gradient, so we may increase the learning rate to make a larger progress along the op- posite of the gradient direction.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Learning rate warmup&lt;/strong&gt;: Using a too large learning rate may result in numerical instability, so we gradually increase it.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Zero \(\gamma\)&lt;/strong&gt;: we initialize \(\gamma = 0\) for all BN layers that sit at the end of a residual block. Therefore, all residual blocks just return their inputs, mimics network that has less number of layers and is easier to train at the initial stage.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;No bias decay&lt;/strong&gt;: Only apply to the regularization to weights to avoid overfitting.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Low-precision training:
    &lt;ul&gt;
      &lt;li&gt;Use FP16 to train since new hardware may have enhanced arithmetic logic unit for lower precision data types.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/efficient_training.png&quot; style=&quot;width: 85%;&quot; alt=&quot;efficient_training&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;model-tweaks&quot;&gt;Model Tweaks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A model tweak is a minor adjustment to the network architecture (changing the stride of convlution layer).&lt;/li&gt;
  &lt;li&gt;Such a tweak barely changes the computational complexity but might have a non-negligible effect on the model accuracy.&lt;/li&gt;
  &lt;li&gt;Take a closer look at Resnet architecture:
    &lt;ul&gt;
      &lt;li&gt;Resnet network consists of an input stem (reduces the input W/H by 4 times and increases its channel size to 64), four subsequent stages and a final output layer.&lt;/li&gt;
      &lt;li&gt;Resnet has a basic block called Bottleneck Block (downsampling block). There are 2 paths in the block:
        &lt;ul&gt;
          &lt;li&gt;Path A: has three convolutions, whose kernel sizes are 1√ó1 (stride 2 - halve the input size), 3√ó3 and 1√ó1 (output channels x4 the previous 2), respectively.&lt;/li&gt;
          &lt;li&gt;Path B: uses a 1√ó1 convolution (stride of 2) to create the same output shape as path A, so we can sum outputs of both paths to obtain the output of the downsampling block.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/resnet.png&quot; style=&quot;width: 50%;&quot; alt=&quot;resnet&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are 3 tweaks mentioned is this paper:&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/resnet_tweak.png&quot; style=&quot;width: 50%;&quot; alt=&quot;resnet_tweak&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Resnet-B&lt;/strong&gt;: modifies downsampling block
    &lt;ul&gt;
      &lt;li&gt;Observation: convolution in path A (original) ignores \(\frac{3}{4}\) of the input feature map because it uses a kernel size 1√ó1 with a stride of 2.&lt;/li&gt;
      &lt;li&gt;Tweak: Change stride of the first two conv, so no information is ignored.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resnet-C&lt;/strong&gt;: modifies input stem
    &lt;ul&gt;
      &lt;li&gt;Observation: the computational cost of a convolution is quadratic to the kernel width or height (A 7 √ó 7 convolution is 5.4 times more expensive than a 3 √ó 3 convolution).&lt;/li&gt;
      &lt;li&gt;Tweak: replacing the 7√ó7 convolution with 3 conservative 3 √ó 3 convolutions, with the first and second convolutions have their output channel of 32 and a stride of 2, while the last convolution uses a 64 output channel.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resnet-D&lt;/strong&gt;: modifies downsampling block
    &lt;ul&gt;
      &lt;li&gt;Observation: Inspired by Resnet-B, 1√ó1 convolution in the path B also ignores \(\frac{3}{4}\) of input feature maps.&lt;/li&gt;
      &lt;li&gt;Tweak: Change stride to 1 and add a 2√ó2 average pooling layer with a stride of 2 before the convolution, since it works well in practice and impacts the computational cost little.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/resnet_tweak_res.png&quot; style=&quot;width: 50%;&quot; alt=&quot;resnet_tweak_res&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;training-refinements&quot;&gt;Training Refinements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Cosine Learning Rate Decay&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Method: Same as my previous post.&lt;/li&gt;
      &lt;li&gt;Result: Compared to the step decay, the cosine decay starts to decay the learning since the beginning but remains large until step decay reduces the learning rate by 10x, which potentially improves the training progress.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/cosanneal.png&quot; style=&quot;width: 50%;&quot; alt=&quot;cosanneal&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Label Smoothing&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Method: Same as my previous post.&lt;/li&gt;
      &lt;li&gt;Result: Compared to the softmax, it encourages a finite output from the fully-connected layer and can generalize better.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Knowledge Distillation&lt;/strong&gt;: skipped.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mixup&lt;/strong&gt;: Same as my previous post.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/bot/refinements.png&quot; style=&quot;width: 50%;&quot; alt=&quot;refinements&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/xl3Bs2qfS1jDOSHA8V/giphy.gif&quot; alt=&quot;rating&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;papers-needs-to-conquer-next-&quot;&gt;Papers needs to conquer next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Focal Loss&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">Bag of Tricks for Image Classification with Convolutional Neural Networks He, Tong Zhang, Zhi Zhang, Hang Zhang, Zhongyue Xie, Junyuan Li, Mu, 2019 Research Topic Category (General): Deep Learning Category (Specific): Computer Vision Paper summary Synthesize multiple effective methods that are briefly mentioned in litterature, in order to evaluate their impact on the final model accuracy through ablation study. Issues addressed by the paper Find a combination of training procedure and model architecture refinements that improve model accuracy but barely change computational complexity. These ‚Äútricks‚Äù usually are minor ones (such as changing the stride of a convolutional layer), but create a big impact. These methods lead to significant accuracy improvement and combining them together can further boost the model accuracy. Transfer learning‚Äôs performance of these ‚Äúboosted‚Äù model would be much better too. Approach/Method Training baseline Use the same training transformation (random sampling, random crop, random flip, hue/saturation/brightness, PCA noise, normalize). Only resize in validation set. Xavier initialization for weights of both convolutional and fully connected. Use NAG (Nesterov), 120 epochs, 8 GPUs, batch-size = 256. LR initialized to 0.1 and divided by 10 at the 30th, 60th, and 90th epochs. Efficient Training Larger training batch-size: Since we‚Äôre using mini-batch SGD, for the same number of epochs, training with a large batch size results in a model with degraded validation accuracy compared to the ones trained with smaller batch sizes. Some heuristics the help scale the batch size up like: Linear scaling learning rate: a large batch size reduces the noise in the gradient, so we may increase the learning rate to make a larger progress along the op- posite of the gradient direction. Learning rate warmup: Using a too large learning rate may result in numerical instability, so we gradually increase it. Zero \(\gamma\): we initialize \(\gamma = 0\) for all BN layers that sit at the end of a residual block. Therefore, all residual blocks just return their inputs, mimics network that has less number of layers and is easier to train at the initial stage. No bias decay: Only apply to the regularization to weights to avoid overfitting. Low-precision training: Use FP16 to train since new hardware may have enhanced arithmetic logic unit for lower precision data types. Model Tweaks A model tweak is a minor adjustment to the network architecture (changing the stride of convlution layer). Such a tweak barely changes the computational complexity but might have a non-negligible effect on the model accuracy. Take a closer look at Resnet architecture: Resnet network consists of an input stem (reduces the input W/H by 4 times and increases its channel size to 64), four subsequent stages and a final output layer. Resnet has a basic block called Bottleneck Block (downsampling block). There are 2 paths in the block: Path A: has three convolutions, whose kernel sizes are 1√ó1 (stride 2 - halve the input size), 3√ó3 and 1√ó1 (output channels x4 the previous 2), respectively. Path B: uses a 1√ó1 convolution (stride of 2) to create the same output shape as path A, so we can sum outputs of both paths to obtain the output of the downsampling block. There are 3 tweaks mentioned is this paper: Resnet-B: modifies downsampling block Observation: convolution in path A (original) ignores \(\frac{3}{4}\) of the input feature map because it uses a kernel size 1√ó1 with a stride of 2. Tweak: Change stride of the first two conv, so no information is ignored. Resnet-C: modifies input stem Observation: the computational cost of a convolution is quadratic to the kernel width or height (A 7 √ó 7 convolution is 5.4 times more expensive than a 3 √ó 3 convolution). Tweak: replacing the 7√ó7 convolution with 3 conservative 3 √ó 3 convolutions, with the first and second convolutions have their output channel of 32 and a stride of 2, while the last convolution uses a 64 output channel. Resnet-D: modifies downsampling block Observation: Inspired by Resnet-B, 1√ó1 convolution in the path B also ignores \(\frac{3}{4}\) of input feature maps. Tweak: Change stride to 1 and add a 2√ó2 average pooling layer with a stride of 2 before the convolution, since it works well in practice and impacts the computational cost little. Training Refinements Cosine Learning Rate Decay: Method: Same as my previous post. Result: Compared to the step decay, the cosine decay starts to decay the learning since the beginning but remains large until step decay reduces the learning rate by 10x, which potentially improves the training progress. Label Smoothing: Method: Same as my previous post. Result: Compared to the softmax, it encourages a finite output from the fully-connected layer and can generalize better. Knowledge Distillation: skipped. Mixup: Same as my previous post. Conclusions Rating Papers needs to conquer next üëèüëèüëè Focal Loss</summary></entry><entry><title type="html">Paper recap: CutMix</title><link href="https://trung-dt.com/paper-recap/2020/08/31/cutmix.html" rel="alternate" type="text/html" title="Paper recap: CutMix" /><published>2020-08-31T00:00:00+07:00</published><updated>2020-08-31T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/08/31/cutmix</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/08/31/cutmix.html">&lt;h2 id=&quot;cutmix-regularization-strategy-to-train-strong-classifiers-with-localizable-features&quot;&gt;CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;Yun, Sangdoo Han, Dongyoon Chun, Sanghyuk Oh, Seong Joon Choe, Junsuk Yoo, Youngjoon&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning&lt;/li&gt;
  &lt;li&gt;Category (Specific): Data Augmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce a new data augmentation method: CutMix, which helps:
    &lt;ul&gt;
      &lt;li&gt;Generates new samples by cutting and pasting patches within mini-batches, leading to performance boosts in many computer vision tasks.&lt;/li&gt;
      &lt;li&gt;Improves the model robustness against input corruptions&lt;/li&gt;
      &lt;li&gt;Performs better when facing out-of-distribution detection problems.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-like-im-5-eli5-&quot;&gt;Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/cutmix/example.jpeg&quot; style=&quot;width: 50%;&quot; alt=&quot;ex&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Regional cutout may remove informative pixels, in addition to it, these techniques also add noises by replace those cut pixels with black area.&lt;/li&gt;
  &lt;li&gt;How can we maximally utilize the deleted regions, while taking advantage of better generalization and localization using regional dropout?&lt;/li&gt;
  &lt;li&gt;While certainly improving classification performance, Mixup samples tend to be unnatural.&lt;/li&gt;
  &lt;li&gt;Synthesizing training data guides also improves model performance, but introduces costly computations.&lt;/li&gt;
  &lt;li&gt;Recently, methods adding noises to the internal features of CNNs or adding extra path to the architecture have been proposed to enhance image classification performance.&lt;/li&gt;
  &lt;li&gt;Cutmix has properties such that can compensate all of those mentioned above.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Core idea: We have 2 pictures, choose a region (apply to both pictures) in the image that is proportional to the image‚Äôs ratio, then swap those regions, leads to a new sample.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;new_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MASK&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MASK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;new_label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;lambda&lt;/code&gt; is sampled from symmetric Beta distribution (having same \(\alpha\) and \(\beta\) value), which looks like the following image. This distribution makes sure that the new image will most of the time be close only to the first or the second picture. Only sometimes both images have the same intensity (of course if in a case of multicategorical, other label will be 0).&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixup/beta.png&quot; style=&quot;width: 50%;&quot; alt=&quot;lr&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;MASK&lt;/code&gt; has the same WxH like the original image, having full 0s in the region that is gonna be cutout (let‚Äôs call it &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;B&lt;/code&gt;) and full 1s in the leftover pixels.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;B&lt;/code&gt; is a bounding box, having coordinates defined as \(B = (r_x, r_y, r_w, r_h)\) indicating the cropping regions.&lt;/li&gt;
&lt;/ul&gt;

\[r_x \sim Unif(0,W); \: r_w = W \sqrt{1 - \lambda} \\
    r_y \sim Unif(0,H); \: r_h = H \sqrt{1 - \lambda}\]

&lt;ul&gt;
  &lt;li&gt;Having these coordinates defined like this would make the cropped area ratio equals to \(1 - \lambda\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/cutmix/compare.png&quot; style=&quot;width: 50%;&quot; alt=&quot;compare&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CutMix overcomes the problem of both Mixup (its samples tend to be unnatural) and regional cutout (removes informative pixels) by replacing the image region with a patch from another training image.&lt;/li&gt;
  &lt;li&gt;CutMix incurs only negligible additional cost for training.&lt;/li&gt;
  &lt;li&gt;Cutout successfully lets a model focus on less discriminative parts of the object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/cutmix/CAM.png&quot; style=&quot;width: 50%;&quot; alt=&quot;CAM&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Outperforms other SOTAs using data augmentation during the time of publish.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/cutmix/result.png&quot; style=&quot;width: 50%;&quot; alt=&quot;result&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;best-practice&quot;&gt;Best practice&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;\(\alpha = 1\) usually performs well.&lt;/li&gt;
  &lt;li&gt;Cutout, Mixup, and CutMix require a greater number of training epochs till convergence.&lt;/li&gt;
  &lt;li&gt;CutMix achieves the best performance when it is applied on the input images rather than hidden features.&lt;/li&gt;
  &lt;li&gt;A new state-of-the-art performance 13.81% (in CIFAR-100) by combining CutMix and ShakeDrop, a regularization that adds noise on intermediate features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hidden-gems&quot;&gt;Hidden gemsüíéüíéüíé&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Label-smoothing is good when apply together with data augmentation such as Cutout, Mixup, Dropblock.&lt;/li&gt;
  &lt;li&gt;Mixup, Manifold Mixup achieve higher accuracies when Cutout is applied on input images.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/l2JeeiVJzPmuMIRlS/giphy.gif&quot; alt=&quot;rating&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;paper-implementation&quot;&gt;Paper implementation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Using Pytorch Lightning Callback system, inspired by fastai implementation.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MixLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoneReduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lerp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduce_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;getattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;reduction&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;mean&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# Cell
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CutmixDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distrib&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_train_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;hasattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;loss_func&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;Your LightningModule should have loss_func attribute as your loss function.&apos;&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fnc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fnc&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_cutmix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;train&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

          &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distrib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;# Permute the batch
&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randperm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;xb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

          &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;_cutmix&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;grid_g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;_cut_from&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;grid_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;_dif&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_train_batch_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataloader_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_cutmix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_validation_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_validation_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fnc&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_fit_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rand_bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;cut_rat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;cut_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cut_rat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;cut_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cut_rat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# uniform
&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cut_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cut_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cut_w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cut_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;papers-needs-to-conquer-next-&quot;&gt;Papers needs to conquer next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Any recommendation?&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features Yun, Sangdoo Han, Dongyoon Chun, Sanghyuk Oh, Seong Joon Choe, Junsuk Yoo, Youngjoon Research Topic Category (General): Deep Learning Category (Specific): Data Augmentation Paper summary Introduce a new data augmentation method: CutMix, which helps: Generates new samples by cutting and pasting patches within mini-batches, leading to performance boosts in many computer vision tasks. Improves the model robustness against input corruptions Performs better when facing out-of-distribution detection problems. Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂ Issues addressed by the paper Regional cutout may remove informative pixels, in addition to it, these techniques also add noises by replace those cut pixels with black area. How can we maximally utilize the deleted regions, while taking advantage of better generalization and localization using regional dropout? While certainly improving classification performance, Mixup samples tend to be unnatural. Synthesizing training data guides also improves model performance, but introduces costly computations. Recently, methods adding noises to the internal features of CNNs or adding extra path to the architecture have been proposed to enhance image classification performance. Cutmix has properties such that can compensate all of those mentioned above. Approach/Method Core idea: We have 2 pictures, choose a region (apply to both pictures) in the image that is proportional to the image‚Äôs ratio, then swap those regions, leads to a new sample. new_image = MASK * image_1 + (1-MASK) * image_2 new_label = lambda * label_1 + (1-lambda) * label_2 lambda is sampled from symmetric Beta distribution (having same \(\alpha\) and \(\beta\) value), which looks like the following image. This distribution makes sure that the new image will most of the time be close only to the first or the second picture. Only sometimes both images have the same intensity (of course if in a case of multicategorical, other label will be 0). MASK has the same WxH like the original image, having full 0s in the region that is gonna be cutout (let‚Äôs call it B) and full 1s in the leftover pixels. B is a bounding box, having coordinates defined as \(B = (r_x, r_y, r_w, r_h)\) indicating the cropping regions. \[r_x \sim Unif(0,W); \: r_w = W \sqrt{1 - \lambda} \\ r_y \sim Unif(0,H); \: r_h = H \sqrt{1 - \lambda}\] Having these coordinates defined like this would make the cropped area ratio equals to \(1 - \lambda\). Result CutMix overcomes the problem of both Mixup (its samples tend to be unnatural) and regional cutout (removes informative pixels) by replacing the image region with a patch from another training image. CutMix incurs only negligible additional cost for training. Cutout successfully lets a model focus on less discriminative parts of the object. Outperforms other SOTAs using data augmentation during the time of publish. Best practice \(\alpha = 1\) usually performs well. Cutout, Mixup, and CutMix require a greater number of training epochs till convergence. CutMix achieves the best performance when it is applied on the input images rather than hidden features. A new state-of-the-art performance 13.81% (in CIFAR-100) by combining CutMix and ShakeDrop, a regularization that adds noise on intermediate features. Hidden gemsüíéüíéüíé Label-smoothing is good when apply together with data augmentation such as Cutout, Mixup, Dropblock. Mixup, Manifold Mixup achieve higher accuracies when Cutout is applied on input images. Conclusions Rating Paper implementation Using Pytorch Lightning Callback system, inspired by fastai implementation. class MixLoss(nn.Module): def __init__(self, old_lf, mixup_cb): super().__init__() self.old_lf = old_lf self.mixup_cb = mixup_cb def forward(self, pred, yb): if self.mixup_cb.pl_module.testing: return self.old_lf(pred, yb) with NoneReduce(self.old_lf) as lf: self.mixup_cb.yb_1 = self.mixup_cb.yb_1.to(pred.device) self.mixup_cb.lam = self.mixup_cb.lam.to(pred.device) loss = torch.lerp(lf(pred, self.mixup_cb.yb_1), lf(pred,yb), self.mixup_cb.lam) return reduce_loss(loss, getattr(self.old_lf, &apos;reduction&apos;, &apos;mean&apos;)) # Cell class CutmixDict(Callback): def __init__(self, alpha=1.): super().__init__() self.distrib = Beta(tensor(alpha), tensor(alpha)) def on_train_start(self, trainer, pl_module): assert hasattr(pl_module, &apos;loss_func&apos;), &apos;Your LightningModule should have loss_func attribute as your loss function.&apos; self.old_lf = pl_module.loss_func self.loss_fnc = MixLoss(self.old_lf, self) pl_module.loss_func = self.loss_fnc self.pl_module = pl_module def _cutmix(self, batch, logger, log_image=False, pre_fix=&apos;train&apos;): xb, yb = batch[&quot;img&quot;], batch[&quot;label&quot;] bs = yb.size(0) W, H = xb.size(3), xb.size(2) lam = self.distrib.sample((1,)).squeeze() lam = torch.stack([lam, 1-lam]) self.lam = lam.max() # Permute the batch shuffle = torch.randperm(bs) xb_1, self.yb_1 = xb[shuffle], yb[shuffle] x1, y1, x2, y2 = self.rand_bbox(W, H, self.lam) xb[:, :, x1:x2, y1:y2] = xb_1[:, :, x1:x2, y1:y2] self.lam = (1 - ((x2-x1) * (y2-y1)) / float(W*H)) if log_image: grid = torchvision.utils.make_grid(xb) logger.experiment.add_image(pre_fix + &apos;_cutmix&apos;, grid) grid_g = torchvision.utils.make_grid(xb_1) logger.experiment.add_image(pre_fix + &apos;_cut_from&apos;, grid_g) dif = abs(xb - xb_1) grid_d = torchvision.utils.make_grid(dif) logger.experiment.add_image(pre_fix + &apos;_dif&apos;, grid_d) return xb def on_train_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx): x = self._cutmix(batch, trainer.logger) batch[&quot;img&quot;] = x def on_validation_start(self, trainer, pl_module): pl_module.loss_func = self.old_lf def on_validation_end(self, trainer, pl_module): pl_module.loss_func = self.loss_fnc def on_fit_end(self, trainer, pl_module): pl_module.loss_func = self.old_lf def rand_bbox(self, W, H, lam): cut_rat = torch.sqrt(1. - lam) cut_w = (W * cut_rat).type(torch.long) cut_h = (H * cut_rat).type(torch.long) # uniform cx = torch.randint(0, W, (1,)) cy = torch.randint(0, H, (1,)) x1 = torch.clamp(cx - cut_w // 2, 0, W) y1 = torch.clamp(cy - cut_h // 2, 0, H) x2 = torch.clamp(cx + cut_w // 2, 0, W) y2 = torch.clamp(cy + cut_h // 2, 0, H) return x1, y1, x2, y2 Papers needs to conquer next üëèüëèüëè Any recommendation?</summary></entry><entry><title type="html">Paper recap: MixUp: Beyond empirical risk minimization</title><link href="https://trung-dt.com/paper-recap/2020/08/24/mixup.html" rel="alternate" type="text/html" title="Paper recap: MixUp: Beyond empirical risk minimization" /><published>2020-08-24T00:00:00+07:00</published><updated>2020-08-24T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/08/24/mixup</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/08/24/mixup.html">&lt;h2 id=&quot;mixup-beyond-empirical-risk-minimization&quot;&gt;MixUp: Beyond empirical risk minimization&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;Zhang, Hongyi Cisse, Moustapha Dauphin, Yann N.Lopez-Paz, David, 2018&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning&lt;/li&gt;
  &lt;li&gt;Category (Specific): Data Augmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce a new data augmentation method: MixUp, which helps:
    &lt;ul&gt;
      &lt;li&gt;Improve generalization of neural network architectures.&lt;/li&gt;
      &lt;li&gt;Reduces memorization of corrupt labels.&lt;/li&gt;
      &lt;li&gt;Increases the robustness to adversarial learning.&lt;/li&gt;
      &lt;li&gt;Stabilize the training of GANs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-like-im-5-eli5-&quot;&gt;Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Well this gif sums up pretty well‚Ä¶
&lt;img src=&quot;https://media.giphy.com/media/BHeCjdyGJck6c/giphy.gif&quot; alt=&quot;pineapple-pen&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;As the model is being trained by minimizing the loss/objective function using a known set of training data (Empirical Risk Minimization), it can choose to &lt;em&gt;memorize&lt;/em&gt; the training data rather than &lt;em&gt;generalize&lt;/em&gt; it, even when in the presence of strong regularization. So the model will perform terribly when evaluating on examples outside the training distribution.&lt;/li&gt;
  &lt;li&gt;Normal data augmentation can help model to improve generalization, but it requires costly computations, and also needs human experts to decide how to augment the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Core idea: Linear interpolate (lerp) two images and its labels together, then do it for the whole batch. Use soft label for better performance. How much to interpolate is used by a number call &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;lambda&lt;/code&gt; generated by Beta Distribution.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;new_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_2&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;new_label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;lambda&lt;/code&gt; is sampled from symmetric Beta distribution (having same \(\alpha\) and \(\beta\) value), which looks like the following image. This distribution makes sure that the new image will most of the time be close only to the first or the second picture. Only sometimes both images have the same intensity (of course if in a case of multicategorical, other label will be 0).&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixup/beta.png&quot; style=&quot;width: 50%;&quot; alt=&quot;lr&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They tried multiple ways to pick images:
    &lt;ul&gt;
      &lt;li&gt;Use 2 different dataloader, take one of each then interpolate -&amp;gt; good mix, but need 2 loaders which takes more time to run.&lt;/li&gt;
      &lt;li&gt;Use more than two pictures to mix: non-significant improvement, but increases the computation cost.&lt;/li&gt;
      &lt;li&gt;Use only 1 dataloader, then mixup the batch and its shuffled-version -&amp;gt; may have duplicates, but peforms equally well -&amp;gt; &lt;strong&gt;RECOMMEND&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Reduces the amount of undesirable oscillations when predicting outside the training examples, also makes memorization more difficult to achieve (generalizes better). The author had make tests by training both normal (ERM) and mixup model against randomly corruptled labels:
    &lt;ul&gt;
      &lt;li&gt;As you can see, the ERM model starts to overfit the corrupted labels when the LR starts to slow down to fine tune.&lt;/li&gt;
      &lt;li&gt;mixup model doesn‚Äôt do the same, so when it‚Äôs tested against the real label, it still performs fairly well -&amp;gt; not overfitted.
&lt;img src=&quot;/assets/images/mixup/generalize.png&quot; alt=&quot;generalize&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Leads to decision boundaries that transition linearly from class to class, providing a smoother estimate of uncertainty:
    &lt;ul&gt;
      &lt;li&gt;In the ERM model, if an adversarial example (class orange) can push through the blue boundary, the model will definately determine that example is class 0.&lt;/li&gt;
      &lt;li&gt;On the other hand, since mixup model produce a smooth boundary, it may still classify that example correctly.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixup/boundaries.png&quot; style=&quot;width: 75%;&quot; alt=&quot;lr&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Significantly improve the robustness of neural networks without hindering the speed of ERM.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;best-practice&quot;&gt;Best practice&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;\(\alpha \in [0.1, 0.4]\) usually performs well, whereas larger \(\alpha\) may lead to underfitting.&lt;/li&gt;
  &lt;li&gt;When using mixup, use larger neural network and also train longer for better result.&lt;/li&gt;
  &lt;li&gt;mixup + dropout performs very nicely together (produced SOTA in the paper).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hidden-gems&quot;&gt;Hidden gemsüíéüíéüíé&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;For normal data augmentation, one usually uses rotation, translation, cropping, resizing, flipping and random erasing to enforce visually plausible invariances in the model through the training data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/8xfNjg3mJPj68/giphy.gif&quot; alt=&quot;rating&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;paper-implementation&quot;&gt;Paper implementation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Using Pytorch Lightning Callback system, inspired by fastai implementation.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MixLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoneReduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lerp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mixup_cb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reduce_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;getattr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;reduction&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;mean&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MixupDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distrib&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_train_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fnc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fnc&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_mixup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;train&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;# Produce &quot;bs&quot; probability for each sample
&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distrib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;# Get those probability that &amp;gt;0.5, so that the first img (in the nonshuffle batch) has bigger coeff
&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;# Which avoid duplication mixup
&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

          &lt;span class=&quot;c1&quot;&gt;# Permute the batch
&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randperm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;xb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yb_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;nx_dims&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nx_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lerp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;log_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;mixup&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;grid_g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;norm&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;grid_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre_fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;dif&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_new&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_train_batch_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataloader_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_mixup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_validation_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_validation_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fnc&lt;/span&gt;

      &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_fit_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;pl_module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_lf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;papers-needs-to-conquer-next-&quot;&gt;Papers needs to conquer next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.04899&quot;&gt;Cutmix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">MixUp: Beyond empirical risk minimization Zhang, Hongyi Cisse, Moustapha Dauphin, Yann N.Lopez-Paz, David, 2018 Research Topic Category (General): Deep Learning Category (Specific): Data Augmentation Paper summary Introduce a new data augmentation method: MixUp, which helps: Improve generalization of neural network architectures. Reduces memorization of corrupt labels. Increases the robustness to adversarial learning. Stabilize the training of GANs. Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂ Well this gif sums up pretty well‚Ä¶ Issues addressed by the paper As the model is being trained by minimizing the loss/objective function using a known set of training data (Empirical Risk Minimization), it can choose to memorize the training data rather than generalize it, even when in the presence of strong regularization. So the model will perform terribly when evaluating on examples outside the training distribution. Normal data augmentation can help model to improve generalization, but it requires costly computations, and also needs human experts to decide how to augment the data. Approach/Method Core idea: Linear interpolate (lerp) two images and its labels together, then do it for the whole batch. Use soft label for better performance. How much to interpolate is used by a number call lambda generated by Beta Distribution. new_image = lambda * image_1 + (1-lambda) * image_2 new_label = lambda * label_1 + (1-lambda) * label_2 lambda is sampled from symmetric Beta distribution (having same \(\alpha\) and \(\beta\) value), which looks like the following image. This distribution makes sure that the new image will most of the time be close only to the first or the second picture. Only sometimes both images have the same intensity (of course if in a case of multicategorical, other label will be 0). They tried multiple ways to pick images: Use 2 different dataloader, take one of each then interpolate -&amp;gt; good mix, but need 2 loaders which takes more time to run. Use more than two pictures to mix: non-significant improvement, but increases the computation cost. Use only 1 dataloader, then mixup the batch and its shuffled-version -&amp;gt; may have duplicates, but peforms equally well -&amp;gt; RECOMMEND. Result Reduces the amount of undesirable oscillations when predicting outside the training examples, also makes memorization more difficult to achieve (generalizes better). The author had make tests by training both normal (ERM) and mixup model against randomly corruptled labels: As you can see, the ERM model starts to overfit the corrupted labels when the LR starts to slow down to fine tune. mixup model doesn‚Äôt do the same, so when it‚Äôs tested against the real label, it still performs fairly well -&amp;gt; not overfitted. Leads to decision boundaries that transition linearly from class to class, providing a smoother estimate of uncertainty: In the ERM model, if an adversarial example (class orange) can push through the blue boundary, the model will definately determine that example is class 0. On the other hand, since mixup model produce a smooth boundary, it may still classify that example correctly. Significantly improve the robustness of neural networks without hindering the speed of ERM. Best practice \(\alpha \in [0.1, 0.4]\) usually performs well, whereas larger \(\alpha\) may lead to underfitting. When using mixup, use larger neural network and also train longer for better result. mixup + dropout performs very nicely together (produced SOTA in the paper). Hidden gemsüíéüíéüíé For normal data augmentation, one usually uses rotation, translation, cropping, resizing, flipping and random erasing to enforce visually plausible invariances in the model through the training data. Conclusions Rating Paper implementation Using Pytorch Lightning Callback system, inspired by fastai implementation. class MixLoss(nn.Module): def __init__(self, old_lf, mixup_cb): super().__init__() self.old_lf = old_lf self.mixup_cb = mixup_cb def forward(self, pred, yb): if self.mixup_cb.pl_module.testing: return self.old_lf(pred, yb) with NoneReduce(self.old_lf) as lf: self.mixup_cb.yb_1 = self.mixup_cb.yb_1.to(pred.device) self.mixup_cb.lam = self.mixup_cb.lam.to(pred.device) loss = torch.lerp(lf(pred, self.mixup_cb.yb_1), lf(pred,yb), self.mixup_cb.lam) return reduce_loss(loss, getattr(self.old_lf, &apos;reduction&apos;, &apos;mean&apos;)) class MixupDict(Callback): def __init__(self, alpha=0.4): super().__init__() self.distrib = Beta(tensor(alpha), tensor(alpha)) def on_train_start(self, trainer, pl_module): self.old_lf = pl_module.loss_func self.loss_fnc = MixLoss(self.old_lf, self) pl_module.loss_func = self.loss_fnc self.pl_module = pl_module def _mixup(self, batch, logger, log_image=False, pre_fix=&apos;train&apos;): xb, yb = batch[&quot;img&quot;], batch[&quot;label&quot;] bs = yb.size(0) # Produce &quot;bs&quot; probability for each sample lam = self.distrib.sample((bs,)).squeeze() # Get those probability that &amp;gt;0.5, so that the first img (in the nonshuffle batch) has bigger coeff # Which avoid duplication mixup lam = torch.stack([lam, 1-lam], 1) self.lam = lam.max(1)[0] # Permute the batch shuffle = torch.randperm(bs) xb_1, self.yb_1 = xb[shuffle], yb[shuffle] nx_dims = len(xb.size()) weight = unsqueeze(self.lam, n=nx_dims-1) x_new = torch.lerp(xb_1, xb, weight=weight) if log_image: grid = torchvision.utils.make_grid(x_new) logger.experiment.add_image(pre_fix + &apos;mixup&apos;, grid) grid_g = torchvision.utils.make_grid(xb) logger.experiment.add_image(pre_fix + &apos;norm&apos;, grid_g) dif = abs(xb - x_new) grid_d = torchvision.utils.make_grid(dif) logger.experiment.add_image(pre_fix + &apos;dif&apos;, grid_d) return x_new def on_train_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx): x = self._mixup(batch, trainer.logger) batch[&quot;img&quot;] = x def on_validation_start(self, trainer, pl_module): pl_module.loss_func = self.old_lf def on_validation_end(self, trainer, pl_module): pl_module.loss_func = self.loss_fnc def on_fit_end(self, trainer, pl_module): pl_module.loss_func = self.old_lf Papers needs to conquer next üëèüëèüëè Cutmix</summary></entry><entry><title type="html">Paper recap: SGDR, Super-convergence</title><link href="https://trung-dt.com/paper-recap/2020/08/17/sgdr.html" rel="alternate" type="text/html" title="Paper recap: SGDR, Super-convergence" /><published>2020-08-17T00:00:00+07:00</published><updated>2020-08-17T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/08/17/sgdr</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/08/17/sgdr.html">&lt;h2 id=&quot;sgdr-stochastic-gradient-descent-with-warm-restarts&quot;&gt;SGDR: Stochastic Gradient Descent with Warm Restarts&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Ilya Loshchilov, Frank Hutter, 2016&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;super-convergence-very-fast-training-of-neural-networks-using-large-learning-rates&quot;&gt;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.07120&quot;&gt;Leslie N. Smith, Nicholay Topin, 2017&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning&lt;/li&gt;
  &lt;li&gt;Category (Specific): Hyper Tuning, Optimization&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;

&lt;h3 id=&quot;sgdr&quot;&gt;SGDR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Partial warm restarts improve rate of convergence, often used in gradient-free optimization.&lt;/li&gt;
  &lt;li&gt;Propose a warm restart technique for stochastic gradient descent.&lt;/li&gt;
  &lt;li&gt;Study its performance on CIFAR-10/100&lt;/li&gt;
  &lt;li&gt;Show that this technique improve its anytime performance when training deep neural network:
    &lt;ul&gt;
      &lt;li&gt;SGD with warm restarts requires 2√ó to 4√ó fewer epochs than the currently-used learning rate schedule schemes to achieve comparable or even better results&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;super-convergence&quot;&gt;Super-convergence&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce ‚ÄúSuper-convergence‚Äù term: where neural networks can be train and converge much faster than standard training methods.&lt;/li&gt;
  &lt;li&gt;Propoce a way to achieve super-convergence: one-cycle policy + large learning rate.&lt;/li&gt;
  &lt;li&gt;Use Hessian Free optimization method to produce an estimate of the optimal learning rate.&lt;/li&gt;
  &lt;li&gt;Study its performance on CIFAR-10/100, MNIST, Imagenet with various model.&lt;/li&gt;
  &lt;li&gt;Show that this phenomenon can also happen when the amount of labeled training data is limited but still boost the model performance.&lt;/li&gt;
  &lt;li&gt;Mostly all of these are mentioned in the previous recap, so it will not be discussed more in the sections below.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-like-im-5-eli5-&quot;&gt;Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂&lt;/h2&gt;

&lt;h3 id=&quot;sgdr-1&quot;&gt;SGDR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Just like a normal person works everyday. You start the day with maximum effort, but then time goes by you feel tired and the productivity reduces. You go home, rest. The next day, you‚Äôre recharged and start the cycle once again.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;super-convergence-1&quot;&gt;Super-convergence&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;As a result of CLR, super-convergence is born.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;

&lt;h3 id=&quot;sgdr-2&quot;&gt;SGDR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Despite of the existance of advanced optimization methods like Adam, AdaDelta, SOTA result on CIFAR-10/100, ImageNet still based on SGD with momentum, associated with Resnet model.&lt;/li&gt;
  &lt;li&gt;Current way to get out of the plateau while using SGD is LR scheduler and L2 regularization.&lt;/li&gt;
  &lt;li&gt;They want to break through and produce a new approach to SGD.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;super-convergence-2&quot;&gt;Super-convergence&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;They found a way to train DNN faster and achieve better performance.&lt;/li&gt;
  &lt;li&gt;Large LR regularizes training, so other regularization method should be reduced to maintain optimal balance of optimization&lt;/li&gt;
  &lt;li&gt;Hessian-free optimization method estimates optimal LR, demonstrating that large LR find wide, flat minima.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;

&lt;h3 id=&quot;sgdr-3&quot;&gt;SGDR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SGDR simulates a new warm-started run/restart of SGD after \(T_{i}\) epochs are performed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/sgdr/cos_annealing.png&quot; style=&quot;width: 50%;&quot; alt=&quot;cosann&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Learning rate is calculated by cos annealing function.&lt;/li&gt;
&lt;/ul&gt;

\[\eta_{t}=\eta_{\min }^{i}+\frac{1}{2}\left(\eta_{\max }^{i}-\eta_{\min }^{i}\right)\left(1+\cos \left(\frac{T_{\text {cur}}}{T_{i}} \pi\right)\right)\]

&lt;ul&gt;
  &lt;li&gt;\(\eta\): learning rate.&lt;/li&gt;
  &lt;li&gt;\(T_{cur}\): how many epochs passed since the restart.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(T_{i}\): how mane epochs for a restart, you can leave it constant or increase over-time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;How the learning rate looks after training:&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/sgdr/sgdr.png&quot; style=&quot;width: 50%;&quot; alt=&quot;sgdr&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;best-practice&quot;&gt;Best practice&lt;/h2&gt;

&lt;h3 id=&quot;sgdr-4&quot;&gt;SGDR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Start with small \(T_{i}\), then increase it by factor of \(T_{mult}\) at every start.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/sgdr/sgdrs.png&quot; style=&quot;width: 100%;&quot; alt=&quot;sgdrs&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Decreate max_lr and min_lr at every new start may increase performance.&lt;/li&gt;
  &lt;li&gt;Stop training when current learning rate is equal to min_lr.&lt;/li&gt;
  &lt;li&gt;SGDR allows to train larger network.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/sgdr/sgdr_res.png&quot; style=&quot;width: 100%;&quot; alt=&quot;sgdres&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SGDR technique helps the author to surpass the SOTA at the time with much faster computational time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;the-authors-conclusions&quot;&gt;The author‚Äôs conclusions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Our SGDR simulates warm restarts by scheduling the learning rate to achieve competitive results on CIFAR-10 and CIFAR-100 roughly two to four times faster, achieved new state- of-the-art results with SGDR.&lt;/li&gt;
  &lt;li&gt;SGDR might also reduce the problem of learning rate selection because the annealing and restarts of SGDR scan / consider a range of learning rate values.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/z8rEcJ6I0hiUM/giphy.gif&quot; alt=&quot;rating&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;my-conclusion&quot;&gt;My Conclusion&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Another technique can be considered when training in new project, should quick test overall before stick to one and go deeper.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-implementation&quot;&gt;Paper implementation&lt;/h2&gt;

&lt;h3 id=&quot;cos-annealing&quot;&gt;Cos Annealing&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sched_cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;sgdr-5&quot;&gt;SGDR&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_0&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_mult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_0&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sched_cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_cur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;cited-references-and-used-images-from&quot;&gt;Cited references and used images from:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1608.03983&lt;/li&gt;
  &lt;li&gt;Pytorch library&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;papers-needs-to-conquer-next-&quot;&gt;Papers needs to conquer next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;Mixup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.04899&quot;&gt;Cutmix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">SGDR: Stochastic Gradient Descent with Warm Restarts Ilya Loshchilov, Frank Hutter, 2016 Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates Leslie N. Smith, Nicholay Topin, 2017 Research Topic Category (General): Deep Learning Category (Specific): Hyper Tuning, Optimization Paper summary SGDR Partial warm restarts improve rate of convergence, often used in gradient-free optimization. Propose a warm restart technique for stochastic gradient descent. Study its performance on CIFAR-10/100 Show that this technique improve its anytime performance when training deep neural network: SGD with warm restarts requires 2√ó to 4√ó fewer epochs than the currently-used learning rate schedule schemes to achieve comparable or even better results Super-convergence Introduce ‚ÄúSuper-convergence‚Äù term: where neural networks can be train and converge much faster than standard training methods. Propoce a way to achieve super-convergence: one-cycle policy + large learning rate. Use Hessian Free optimization method to produce an estimate of the optimal learning rate. Study its performance on CIFAR-10/100, MNIST, Imagenet with various model. Show that this phenomenon can also happen when the amount of labeled training data is limited but still boost the model performance. Mostly all of these are mentioned in the previous recap, so it will not be discussed more in the sections below. Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂ SGDR Just like a normal person works everyday. You start the day with maximum effort, but then time goes by you feel tired and the productivity reduces. You go home, rest. The next day, you‚Äôre recharged and start the cycle once again. Super-convergence As a result of CLR, super-convergence is born. Issues addressed by the paper SGDR Despite of the existance of advanced optimization methods like Adam, AdaDelta, SOTA result on CIFAR-10/100, ImageNet still based on SGD with momentum, associated with Resnet model. Current way to get out of the plateau while using SGD is LR scheduler and L2 regularization. They want to break through and produce a new approach to SGD. Super-convergence They found a way to train DNN faster and achieve better performance. Large LR regularizes training, so other regularization method should be reduced to maintain optimal balance of optimization Hessian-free optimization method estimates optimal LR, demonstrating that large LR find wide, flat minima. Approach/Method SGDR SGDR simulates a new warm-started run/restart of SGD after \(T_{i}\) epochs are performed. Learning rate is calculated by cos annealing function. \[\eta_{t}=\eta_{\min }^{i}+\frac{1}{2}\left(\eta_{\max }^{i}-\eta_{\min }^{i}\right)\left(1+\cos \left(\frac{T_{\text {cur}}}{T_{i}} \pi\right)\right)\] \(\eta\): learning rate. \(T_{cur}\): how many epochs passed since the restart. \(T_{i}\): how mane epochs for a restart, you can leave it constant or increase over-time. How the learning rate looks after training: Best practice SGDR Start with small \(T_{i}\), then increase it by factor of \(T_{mult}\) at every start. Decreate max_lr and min_lr at every new start may increase performance. Stop training when current learning rate is equal to min_lr. SGDR allows to train larger network. Results SGDR technique helps the author to surpass the SOTA at the time with much faster computational time. Conclusions The author‚Äôs conclusions Our SGDR simulates warm restarts by scheduling the learning rate to achieve competitive results on CIFAR-10 and CIFAR-100 roughly two to four times faster, achieved new state- of-the-art results with SGDR. SGDR might also reduce the problem of learning rate selection because the annealing and restarts of SGDR scan / consider a range of learning rate values. Rating My Conclusion Another technique can be considered when training in new project, should quick test overall before stick to one and go deeper. Paper implementation Cos Annealing def sched_cos(start, end, T_cur, T_i): return start + (1 + math.cos(math.pi*(T_cur / T_i))) * (end-start) / 2 SGDR def step(self, epoch=None): if epoch is None and self.last_epoch &amp;lt; 0: epoch = 0 if epoch is None: epoch = self.last_epoch + 1 self.T_cur = self.T_cur + 1 if self.T_cur &amp;gt;= self.T_i: self.T_cur = self.T_cur - self.T_i self.T_i = self.T_i * self.T_mult else: if epoch &amp;lt; 0: return if epoch &amp;gt;= self.T_0: if self.T_mult == 1: self.T_cur = epoch % self.T_0 else: n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult)) self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1) self.T_i = self.T_0 * self.T_mult ** (n) else: self.T_i = self.T_0 self.T_cur = epoch self.last_epoch = math.floor(epoch) self.lr = sched_cos(self.base_lr, self.max_lr, self.T_cur, self.T_i) Cited references and used images from: https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163 https://arxiv.org/abs/1608.03983 Pytorch library Papers needs to conquer next üëèüëèüëè Mixup Cutmix</summary></entry><entry><title type="html">Paper recap: A disciplined approach to neural network hyper-parameters: Part 1</title><link href="https://trung-dt.com/paper-recap/2020/08/10/1cycle.html" rel="alternate" type="text/html" title="Paper recap: A disciplined approach to neural network hyper-parameters: Part 1" /><published>2020-08-10T00:00:00+07:00</published><updated>2020-08-10T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/08/10/1cycle</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/08/10/1cycle.html">&lt;h2 id=&quot;a-disciplined-approach-to-neural-network-hyper-parameters-part-1&quot;&gt;A disciplined approach to neural network hyper-parameters: Part 1&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1803.09820&quot;&gt;Lesli N.Smith, 2018&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning.&lt;/li&gt;
  &lt;li&gt;Category (Specific): Hyperparameters Tuning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce techniques to set some essential hyper-parameters such as: learning rate, batch size, momentum, weight decay.&lt;/li&gt;
  &lt;li&gt;How to examine training/ test loss curve for clues of underfitting, overfitting.&lt;/li&gt;
  &lt;li&gt;Introduce a new method of cyclical learning rate: &lt;strong&gt;1cycle policy&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Discuss about cyclical momentum, cyclical weight decay.&lt;/li&gt;
  &lt;li&gt;Produce multiples examples to show the importance of balanced regularization for each dataset/ architecture.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-like-im-5-eli5-&quot;&gt;Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Not necessary since this paper mostly tackle best pratices while working on projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Grid-search is computationally expensive and time consumming. But a good choice of hyper-parameters is vital for a model to perform well, so how to do grid-search efficiently?&lt;/li&gt;
  &lt;li&gt;Underfitting/ overfitting trade-off.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Learning rate:
    &lt;ul&gt;
      &lt;li&gt;Too small: overfitting can occur.&lt;/li&gt;
      &lt;li&gt;Too large: can have regularize but will diverge.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Batch size:
    &lt;ul&gt;
      &lt;li&gt;Suggest that when we comparing batch size, neither maintaining &lt;em&gt;constant #epochs&lt;/em&gt; (train the same #epochs for each batch size) nor &lt;em&gt;constant #iterations&lt;/em&gt; (train the same #iterations/epoch each batch size) is appropriate:
        &lt;ul&gt;
          &lt;li&gt;constant #epochs:
            &lt;ul&gt;
              &lt;li&gt;Computationally efficient, but penalized more since we see a big proportion of samples each time.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;constant #iterations:
            &lt;ul&gt;
              &lt;li&gt;Overfit will occur.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Batch size directly affect computational time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Momentum:
    &lt;ul&gt;
      &lt;li&gt;Momentum and learning rate are closely related and its optimal values are dependent on each other.&lt;/li&gt;
      &lt;li&gt;Momentum‚Äôs effect on updating the weights is of the same magnitude as the learning rate (used SGD with momentum equation to verify).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Weight decay:
    &lt;ul&gt;
      &lt;li&gt;Weight decay is one form of regularization and it plays an important role in training so its value needs to be set properly.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;
&lt;!-- ![lr](/assets/images/1cycle/lr.png) --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/1cycle/lr.png&quot; style=&quot;width: 50%;&quot; alt=&quot;lr&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1cycle policy:
    &lt;ul&gt;
      &lt;li&gt;Instead of cyclical learning rate using &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;triangular&lt;/code&gt; as the &lt;a href=&quot;/paper-recap/2020/08/03/clr.html&quot;&gt;previous post&lt;/a&gt;, the author suggest to:
        &lt;ol&gt;
          &lt;li&gt;Train all epochs in more than one cycle just a small proportion.&lt;/li&gt;
          &lt;li&gt;In the remaining iterations, the learning rate will decline from &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt; to several orders of magnitude less.&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;Experiments show that this policy allows the accuracy to plateau before the training ends.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The author show these 6 remarks after multiple researchs and experiments:
    &lt;ol&gt;
      &lt;li&gt;The test/validation loss is a good indicator of the network‚Äôs convergence and should be examined for clues.
        &lt;ul&gt;
          &lt;li&gt;Look at the loss curve and also plot generalization error curve (&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;valid_loss - train_loss&lt;/code&gt;), one can determine whether the architechture has the capacity to overfit or has too small learning rate (which also leads to overfit)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Achieving the horizontal part of the test loss is the goal of hyperparameter tuning.
  &lt;img src=&quot;/assets/images/1cycle/under_over_plot.png&quot; alt=&quot;under_over_plot&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;The horizontal part is the red line.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The amount of regularization must be balanced for each dataset and architecture.&lt;/li&gt;
      &lt;li&gt;The practitioner‚Äôs goal is obtaining the highest performance while minimizing the needed computational time.&lt;/li&gt;
      &lt;li&gt;Optimal momentum value(s) will improve network training.&lt;/li&gt;
      &lt;li&gt;Since the amount of regularization must be balanced for each dataset and architecture, the value of weight decay is a key knob to turn for tuning regularization against the regularization from an increasing learning rate.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;best-practices&quot;&gt;Best practices&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/1cycle/lr_m.png&quot; alt=&quot;lr_m&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learning rate:
    &lt;ul&gt;
      &lt;li&gt;Use learning rate range test to find the minimum and maximum learning rate boundaries:
        &lt;ul&gt;
          &lt;li&gt;Maximum learning rate bound: the maximum value that the model can still converge&lt;/li&gt;
          &lt;li&gt;Minumum learning rate bound:
            &lt;ol&gt;
              &lt;li&gt;\(\frac{1}{3};\frac{1}{4}\) of max bound.&lt;/li&gt;
              &lt;li&gt;\(\frac{1}{10};\frac{1}{20}\) of max bound if using 1cycle.&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Use 1cycle policy to achive super-convergence (reachs global optima with iterations much less than regulars).&lt;/li&gt;
      &lt;li&gt;Other regularization methods must be reduced to compensate for the regularization effects of large learning rates.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Batch size:
    &lt;ul&gt;
      &lt;li&gt;Small batch sizes add regularization, large batch sizes add less; utilize this while balancing the proper amount of regularization.&lt;/li&gt;
      &lt;li&gt;Often better to use a larger batch size so a larger learning rate can be used (leads to using a larger batch size when using the 1cycle learning rate schedule).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Momentum:
    &lt;ul&gt;
      &lt;li&gt;Short runs with momentum values of 0.99, 0.97, 0.95, and 0.9 will quickly show the best value for momentum.&lt;/li&gt;
      &lt;li&gt;If use 1cycle policy, should use cyclical momentum starting at maximum momentum value and decreasing to a value of 0.8 or 0.85 (performance is almost independent of the minimum momentum value).&lt;/li&gt;
      &lt;li&gt;Decreasing cyclical momentum when the learning rate increases provides an equivalent result to the best constant momentum value.&lt;/li&gt;
      &lt;li&gt;Using cyclical momentum along with the LR range test stabilizes the convergence when using large learning rate values more than a constant momentum does.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Weight decay:
    &lt;ul&gt;
      &lt;li&gt;Should be a constant value.&lt;/li&gt;
      &lt;li&gt;Should use grid search to find a proper value; validation loss early in the training is sufficient for determining a good value.&lt;/li&gt;
      &lt;li&gt;Another option as a grid search for weight decay is to make a single run at a middle value for weight decay and save a snapshot after the loss plateaus. Use this snapshot to restart runs, each with a different value of WD. This can save time in searching for the best weight decay.&lt;/li&gt;
      &lt;li&gt;A complex dataset requires less regularization so test smaller weight decay values, such as \(10^{‚àí4}, 10^{‚àí5}, 10^{‚àí6}, 0\)&lt;/li&gt;
      &lt;li&gt;A shallow architecture requires more regularization so test larger weight decay values, such as \(10^{‚àí2}, 10^{‚àí3}, 10^{‚àí4}\).&lt;/li&gt;
      &lt;li&gt;The optimal weight decay is different if you search with a constant learning rate versus using a learning rate range.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;hidden-gems&quot;&gt;Hidden gemsüíéüíéüíé&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/1cycle/test_loss_dec.png&quot; alt=&quot;test_loss_dec&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Test loss decreases more rapidly during the initial iterations and is then horizontal is an early positive clue indicating that the model will produce a better final accuracy. (Blue curve)&lt;/li&gt;
  &lt;li&gt;Learning rates that are too small can exhibit some overfitting behavior.&lt;/li&gt;
  &lt;li&gt;There is a maximum speed the learning rate can increase without the training becoming unstable.&lt;/li&gt;
  &lt;li&gt;The very large learning rates provided the twin benefits of regularization that prevented overfitting and faster training of the network.&lt;/li&gt;
  &lt;li&gt;Set momentum as large as possible without causing instabilities during training.&lt;/li&gt;
  &lt;li&gt;Momentum range test is not useful for finding an optimal momentum, you should use grid search.&lt;/li&gt;
  &lt;li&gt;Decreasing the momentum while the learning rate increases provides three benefits (by experiments):
    &lt;ol&gt;
      &lt;li&gt;a lower minimum test loss.&lt;/li&gt;
      &lt;li&gt;faster initial convergence.&lt;/li&gt;
      &lt;li&gt;greater convergence stability over a larger range of learning rates.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Large momentum helps escape saddle points but can hurt the final convergence, implying that momentum should be reduced at the end of training.&lt;/li&gt;
  &lt;li&gt;A good procedure is to test momentum values in the range of 0.9 to 0.99.&lt;/li&gt;
  &lt;li&gt;All the general ideas can apply to shallow or deep networks, although the details (i.e., specific values for momentum) varied.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/1cycle/result.png&quot; alt=&quot;result&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As the experiment shown, if one can find optimal values for these hyper-parameters, the model would achieve super-convergence, which saves computational cost and time.
    &lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;These disciplines are not proved but mostly achieved by experiments. So we can only use this as a guide and apply it to our projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;confusing-aspects-of-the-paper&quot;&gt;Confusing aspects of the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Very straightforward, confusing-free.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;Would read again.&lt;/p&gt;

&lt;h3 id=&quot;my-conclusion&quot;&gt;My conclusion&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Good workflow to deal with hyper-parameters.&lt;/li&gt;
  &lt;li&gt;Can be use as a reference when start a new projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-implementation&quot;&gt;Paper implementation&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/nKZEvTua5D4o0XD6Ge/giphy.gif&quot; alt=&quot;Nope&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cited-references-and-used-images-from&quot;&gt;Cited references and used images from:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1803.09820&quot;&gt;Lesli N.Smith, 2018&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://sgugger.github.io/the-1cycle-policy.html&lt;/li&gt;
  &lt;li&gt;https://github.com/asvcode/1_cycle&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;papers-needs-to-conquer-next-&quot;&gt;Papers needs to conquer next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">A disciplined approach to neural network hyper-parameters: Part 1 Lesli N.Smith, 2018 Research Topic Category (General): Deep Learning. Category (Specific): Hyperparameters Tuning. Paper summary Introduce techniques to set some essential hyper-parameters such as: learning rate, batch size, momentum, weight decay. How to examine training/ test loss curve for clues of underfitting, overfitting. Introduce a new method of cyclical learning rate: 1cycle policy. Discuss about cyclical momentum, cyclical weight decay. Produce multiples examples to show the importance of balanced regularization for each dataset/ architecture. Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂ Not necessary since this paper mostly tackle best pratices while working on projects. Issues addressed by the paper Grid-search is computationally expensive and time consumming. But a good choice of hyper-parameters is vital for a model to perform well, so how to do grid-search efficiently? Underfitting/ overfitting trade-off. Learning rate: Too small: overfitting can occur. Too large: can have regularize but will diverge. Batch size: Suggest that when we comparing batch size, neither maintaining constant #epochs (train the same #epochs for each batch size) nor constant #iterations (train the same #iterations/epoch each batch size) is appropriate: constant #epochs: Computationally efficient, but penalized more since we see a big proportion of samples each time. constant #iterations: Overfit will occur. Batch size directly affect computational time. Momentum: Momentum and learning rate are closely related and its optimal values are dependent on each other. Momentum‚Äôs effect on updating the weights is of the same magnitude as the learning rate (used SGD with momentum equation to verify). Weight decay: Weight decay is one form of regularization and it plays an important role in training so its value needs to be set properly. Approach/Method 1cycle policy: Instead of cyclical learning rate using triangular as the previous post, the author suggest to: Train all epochs in more than one cycle just a small proportion. In the remaining iterations, the learning rate will decline from base_lr to several orders of magnitude less. Experiments show that this policy allows the accuracy to plateau before the training ends. The author show these 6 remarks after multiple researchs and experiments: The test/validation loss is a good indicator of the network‚Äôs convergence and should be examined for clues. Look at the loss curve and also plot generalization error curve (valid_loss - train_loss), one can determine whether the architechture has the capacity to overfit or has too small learning rate (which also leads to overfit) Achieving the horizontal part of the test loss is the goal of hyperparameter tuning. The horizontal part is the red line. The amount of regularization must be balanced for each dataset and architecture. The practitioner‚Äôs goal is obtaining the highest performance while minimizing the needed computational time. Optimal momentum value(s) will improve network training. Since the amount of regularization must be balanced for each dataset and architecture, the value of weight decay is a key knob to turn for tuning regularization against the regularization from an increasing learning rate. Best practices Learning rate: Use learning rate range test to find the minimum and maximum learning rate boundaries: Maximum learning rate bound: the maximum value that the model can still converge Minumum learning rate bound: \(\frac{1}{3};\frac{1}{4}\) of max bound. \(\frac{1}{10};\frac{1}{20}\) of max bound if using 1cycle. Use 1cycle policy to achive super-convergence (reachs global optima with iterations much less than regulars). Other regularization methods must be reduced to compensate for the regularization effects of large learning rates. Batch size: Small batch sizes add regularization, large batch sizes add less; utilize this while balancing the proper amount of regularization. Often better to use a larger batch size so a larger learning rate can be used (leads to using a larger batch size when using the 1cycle learning rate schedule). Momentum: Short runs with momentum values of 0.99, 0.97, 0.95, and 0.9 will quickly show the best value for momentum. If use 1cycle policy, should use cyclical momentum starting at maximum momentum value and decreasing to a value of 0.8 or 0.85 (performance is almost independent of the minimum momentum value). Decreasing cyclical momentum when the learning rate increases provides an equivalent result to the best constant momentum value. Using cyclical momentum along with the LR range test stabilizes the convergence when using large learning rate values more than a constant momentum does. Weight decay: Should be a constant value. Should use grid search to find a proper value; validation loss early in the training is sufficient for determining a good value. Another option as a grid search for weight decay is to make a single run at a middle value for weight decay and save a snapshot after the loss plateaus. Use this snapshot to restart runs, each with a different value of WD. This can save time in searching for the best weight decay. A complex dataset requires less regularization so test smaller weight decay values, such as \(10^{‚àí4}, 10^{‚àí5}, 10^{‚àí6}, 0\) A shallow architecture requires more regularization so test larger weight decay values, such as \(10^{‚àí2}, 10^{‚àí3}, 10^{‚àí4}\). The optimal weight decay is different if you search with a constant learning rate versus using a learning rate range. Hidden gemsüíéüíéüíé Test loss decreases more rapidly during the initial iterations and is then horizontal is an early positive clue indicating that the model will produce a better final accuracy. (Blue curve) Learning rates that are too small can exhibit some overfitting behavior. There is a maximum speed the learning rate can increase without the training becoming unstable. The very large learning rates provided the twin benefits of regularization that prevented overfitting and faster training of the network. Set momentum as large as possible without causing instabilities during training. Momentum range test is not useful for finding an optimal momentum, you should use grid search. Decreasing the momentum while the learning rate increases provides three benefits (by experiments): a lower minimum test loss. faster initial convergence. greater convergence stability over a larger range of learning rates. Large momentum helps escape saddle points but can hurt the final convergence, implying that momentum should be reduced at the end of training. A good procedure is to test momentum values in the range of 0.9 to 0.99. All the general ideas can apply to shallow or deep networks, although the details (i.e., specific values for momentum) varied. Results As the experiment shown, if one can find optimal values for these hyper-parameters, the model would achieve super-convergence, which saves computational cost and time. Limitations These disciplines are not proved but mostly achieved by experiments. So we can only use this as a guide and apply it to our projects. Confusing aspects of the paper Very straightforward, confusing-free. Conclusions Rating Would read again. My conclusion Good workflow to deal with hyper-parameters. Can be use as a reference when start a new projects. Paper implementation Cited references and used images from: Lesli N.Smith, 2018 https://sgugger.github.io/the-1cycle-policy.html https://github.com/asvcode/1_cycle Papers needs to conquer next üëèüëèüëè SGDR</summary></entry><entry><title type="html">Paper recap: Cyclical Learning Rates for Training Neural Networks</title><link href="https://trung-dt.com/paper-recap/2020/08/03/clr.html" rel="alternate" type="text/html" title="Paper recap: Cyclical Learning Rates for Training Neural Networks" /><published>2020-08-03T00:00:00+07:00</published><updated>2020-08-03T00:00:00+07:00</updated><id>https://trung-dt.com/paper-recap/2020/08/03/clr</id><content type="html" xml:base="https://trung-dt.com/paper-recap/2020/08/03/clr.html">&lt;h2 id=&quot;cyclical-learning-rates-for-training-neural-networks&quot;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01186&quot;&gt;Lesli N.Smith, 2017&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;research-topic&quot;&gt;Research Topic&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Category (General): Deep Learning&lt;/li&gt;
  &lt;li&gt;Category (Specific): Hyperparameters (Learning rate) Tuning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Instead of monotonically decreasing the learning rate as in the traditional way, the author introduced a new method (called Cyclical Learning Rates) to control it, allowing the hyper-parameter to rise and fall systematically during training.&lt;/li&gt;
  &lt;li&gt;CLR improves classification accuracy without tuning and does not require any computations.&lt;/li&gt;
  &lt;li&gt;The author also showed a way to estimate the bounds for the learning rate to vary while training.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-like-im-5-eli5-&quot;&gt;Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂&lt;/h2&gt;
&lt;p&gt;Imagine you try to draw a picture, there are multiple ways to tackle this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You jump straight-in, drawing up to ~80-90% of the picture. However, to refine it, you have to slow down to finish the touches, which very tiring, so you might have to slow down much more until you finish it or abandon it midway. (~ Learning rate schedule)&lt;/li&gt;
  &lt;li&gt;You start by drawing the layout, and then you calculate how much effort you should spend on each section. After you finish one, you reevaluate again and continue to draw until the picture is done. (~Adaptive learning rate)&lt;/li&gt;
  &lt;li&gt;You start simple, draw a tree today, draw some more trees tomorrow, and draw a sun and a mountain the following day. The gist is that you start slow and progress over time until you reach a particular intensity. Then you slow down again to avoid burnout until you‚Äôre at the initial state (finish a cycle), you will restart the cycle again until the picture is done. (~Cyclical Learning Rates)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;issues-addressed-by-the-paper&quot;&gt;Issues addressed by the paper&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;To train a deep neural network to convergence requires one to experiment with a variety of LR.&lt;/li&gt;
  &lt;li&gt;There are already multiple solutions to this. These include learning rate scheduling (e.g., time-based decay, step decay, exponential decay), or adaptive learning rate (e.g., RMSProp, AdaGrad, AdaDelta, Adam), but there are still some drawbacks such as:
    &lt;ul&gt;
      &lt;li&gt;Learning rate scheduling: Monotonically decreased learning rate, which later proved that it would not help the model escape from the saddle point.&lt;/li&gt;
      &lt;li&gt;Adaptive learning rate: requires high computational cost.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approachmethod&quot;&gt;Approach/Method&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Observation: increasing the learning rate might have a short term negative effect and yet achieve a longer-term beneficial effect. (since increasing the learning rate allows more rapid traversal of saddle point plateaus)&lt;/li&gt;
  &lt;li&gt;Pick minimum (&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;) and maximum (&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;max_lr&lt;/code&gt;) boundaries, and the learning rate will cyclically vary between these bounds.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clr/window.png&quot; alt=&quot;Window type&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For the cyclical function, triangular(Bartlett) window, parabolic(Welch) window, and sinusoidal(Hanning) window produced equivalent results, which led to adopting a triangular window thanks to its simplicity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other variations:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;triangular&lt;/code&gt;: using triangular window&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;triangular_2&lt;/code&gt;: same as triangular but after every cycle, &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;max_lr&lt;/code&gt; is halved&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;exp_range&lt;/code&gt;: each boundary value declines by exponential factor of current iterations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clr/triangular.png&quot; alt=&quot;Triangular window&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to find the bounds?
    &lt;ul&gt;
      &lt;li&gt;Using ‚ÄúLR range test‚Äù:
        &lt;ul&gt;
          &lt;li&gt;Run model for several epochs while letting the learning rate increase linearly between low and high LR values.&lt;/li&gt;
          &lt;li&gt;Notice these 2 points:
            &lt;ul&gt;
              &lt;li&gt;When accuracy starts to increase. -&amp;gt; &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;&lt;/li&gt;
              &lt;li&gt;When accuracy slows down, becomes ragged or falls down. -&amp;gt; &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;max_lr&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clr/lr_range.png&quot; alt=&quot;LR_finder&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;best-practices&quot;&gt;Best practices&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Set &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;stepsize&lt;/code&gt; to 2-10 times of #iterations/epoch.&lt;/li&gt;
  &lt;li&gt;Best to stop training at the end of the cycle (LR at &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt; and the accuracy peaks)
    &lt;ul&gt;
      &lt;li&gt;-&amp;gt; Early stopping might not be good for CLR.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimum learning rate is usually within a factor of two of the largest one that converges, and set &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt; = \(\frac{1}{3}\) or \(\frac{1}{4}\)  of &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;max_lr&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/clr/clr_result.png&quot; alt=&quot;Result&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;CLR&lt;/code&gt; helps to model to converge much faster.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;Decay&lt;/code&gt; (monotonically decreasing LR)‚Äôs result provides evidence that both increasing and decreasing LR are essential.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/clr/clr_adaptive_result.png&quot; alt=&quot;Result&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When using with adaptive learning rate methods, the benefits from CLR are reduced.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;confusing-aspects-of-the-paper&quot;&gt;Confusing aspects of the paper&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/clr/triangular_code.png&quot; alt=&quot;Triangular code&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The variables of the code weren‚Äôt explained clearly, so I redefine it here:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;1 epoch&lt;/strong&gt;: converted to #iterations (= training_size / batch_size).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;1 cycle&lt;/strong&gt;: LR goes from &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;max_lr&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;epochCounter&lt;/code&gt;: current #iterations (e.g: 1 epoch has 2000 iters, the model‚Äôs at epochs 1.5 =&amp;gt; &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;epochCounter&lt;/code&gt; = 3000).&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;stepsize&lt;/code&gt;: #iterations to reach 1/2 cycle.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;opt.LR&lt;/code&gt;: base lr&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;cycle&lt;/code&gt;: which cycle the model‚Äôs currently in (1-2-‚Ä¶), always starts from 1 (first cycle).&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;x&lt;/code&gt;: which part of the half-cycle, in range [0,1].&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;lr&lt;/code&gt;: the updated lr, with &lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;triangular&lt;/code&gt; method.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;h3 id=&quot;the-authors-conclusions&quot;&gt;The author‚Äôs conclusions&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;All experiments show similar or better accuracy performance when using CLR versus using a fixed learning rate, even though the performance drops at some of the learning rate values within this range.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rating&quot;&gt;Rating&lt;/h3&gt;
&lt;p&gt;Noice&lt;/p&gt;

&lt;h3 id=&quot;my-conclusion&quot;&gt;My Conclusion&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;This paper is straightforward, well-explained, so I highly recommend new DL-practitioners to try reading it.&lt;/li&gt;
  &lt;li&gt;CLR is an impressive technique to control the learning rate. We should try this method when we started a new model or a new dataset, giving a lovely baseline for further optimization.&lt;/li&gt;
  &lt;li&gt;CLR is also widely used by kagglers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paper-implementation&quot;&gt;Paper implementation&lt;/h2&gt;

&lt;h3 id=&quot;cyclical-learning-rates&quot;&gt;Cyclical Learning Rates&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;triangular&lt;/code&gt; implementation:
&lt;img src=&quot;/assets/images/clr/triangular_code_pt.png&quot; alt=&quot;triangular_code_pt&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;triangular_2&lt;/code&gt; implementation:
&lt;img src=&quot;/assets/images/clr/triangular2_code_pt.png&quot; alt=&quot;triangular2_code_pt&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext inlined highlighter-rouge&quot;&gt;exp_range&lt;/code&gt; implementation:
&lt;img src=&quot;/assets/images/clr/exp_range_code_pt.png&quot; alt=&quot;exp_range_code_pt&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Plot:
&lt;img src=&quot;/assets/images/clr/all_res.png&quot; alt=&quot;all_test&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Explaination for how x is calculated:
    &lt;ul&gt;
      &lt;li&gt;\(raw\_x = \frac{current\_iteration}{step\_size}\): current cycle in term of half-cycles (floatting point)
&lt;img src=&quot;/assets/images/clr/raw_x.png&quot; alt=&quot;raw_x&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;\(x‚Äô = raw\_x - 2*cycle\): how many half-cycles left to complete this cycle
&lt;img src=&quot;/assets/images/clr/x_prime.png&quot; alt=&quot;x_prime&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;\(x‚Äô = x‚Äô + 1\): Shift so that the function 0-centered on the y-axis, so that when we take absolute, we can achieve cycles.
&lt;img src=&quot;/assets/images/clr/x_shifted.png&quot; alt=&quot;x_prime_shift&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-rate-finder&quot;&gt;Learning Rate Finder&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;An implementation of LRFinder using CLR using Callback (highly influenced by fastai):
&lt;img src=&quot;/assets/images/clr/lr_finder.png&quot; alt=&quot;lr_finder&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cited-references-and-used-images-from&quot;&gt;Cited references and used images from:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.jeremyjordan.me/nn-learning-rate/&lt;/li&gt;
  &lt;li&gt;https://github.com/bckenstler/CLR/&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;papers-needs-to-conquer-next-&quot;&gt;Papers needs to conquer next üëèüëèüëè&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.09820&quot;&gt;Fit One Cycle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="paper-recap" /><summary type="html">Cyclical Learning Rates for Training Neural Networks Lesli N.Smith, 2017 Research Topic Category (General): Deep Learning Category (Specific): Hyperparameters (Learning rate) Tuning Paper summary Instead of monotonically decreasing the learning rate as in the traditional way, the author introduced a new method (called Cyclical Learning Rates) to control it, allowing the hyper-parameter to rise and fall systematically during training. CLR improves classification accuracy without tuning and does not require any computations. The author also showed a way to estimate the bounds for the learning rate to vary while training. Explain Like I‚Äôm 5 (ELI5) üë∂üë∂üë∂ Imagine you try to draw a picture, there are multiple ways to tackle this: You jump straight-in, drawing up to ~80-90% of the picture. However, to refine it, you have to slow down to finish the touches, which very tiring, so you might have to slow down much more until you finish it or abandon it midway. (~ Learning rate schedule) You start by drawing the layout, and then you calculate how much effort you should spend on each section. After you finish one, you reevaluate again and continue to draw until the picture is done. (~Adaptive learning rate) You start simple, draw a tree today, draw some more trees tomorrow, and draw a sun and a mountain the following day. The gist is that you start slow and progress over time until you reach a particular intensity. Then you slow down again to avoid burnout until you‚Äôre at the initial state (finish a cycle), you will restart the cycle again until the picture is done. (~Cyclical Learning Rates) Issues addressed by the paper To train a deep neural network to convergence requires one to experiment with a variety of LR. There are already multiple solutions to this. These include learning rate scheduling (e.g., time-based decay, step decay, exponential decay), or adaptive learning rate (e.g., RMSProp, AdaGrad, AdaDelta, Adam), but there are still some drawbacks such as: Learning rate scheduling: Monotonically decreased learning rate, which later proved that it would not help the model escape from the saddle point. Adaptive learning rate: requires high computational cost. Approach/Method Observation: increasing the learning rate might have a short term negative effect and yet achieve a longer-term beneficial effect. (since increasing the learning rate allows more rapid traversal of saddle point plateaus) Pick minimum (base_lr) and maximum (max_lr) boundaries, and the learning rate will cyclically vary between these bounds. For the cyclical function, triangular(Bartlett) window, parabolic(Welch) window, and sinusoidal(Hanning) window produced equivalent results, which led to adopting a triangular window thanks to its simplicity. Other variations: triangular: using triangular window triangular_2: same as triangular but after every cycle, max_lr is halved exp_range: each boundary value declines by exponential factor of current iterations. How to find the bounds? Using ‚ÄúLR range test‚Äù: Run model for several epochs while letting the learning rate increase linearly between low and high LR values. Notice these 2 points: When accuracy starts to increase. -&amp;gt; base_lr When accuracy slows down, becomes ragged or falls down. -&amp;gt; max_lr Best practices Set stepsize to 2-10 times of #iterations/epoch. Best to stop training at the end of the cycle (LR at base_lr and the accuracy peaks) -&amp;gt; Early stopping might not be good for CLR. Optimum learning rate is usually within a factor of two of the largest one that converges, and set base_lr = \(\frac{1}{3}\) or \(\frac{1}{4}\) of max_lr. Results CLR helps to model to converge much faster. Decay (monotonically decreasing LR)‚Äôs result provides evidence that both increasing and decreasing LR are essential. Limitations When using with adaptive learning rate methods, the benefits from CLR are reduced. Confusing aspects of the paper The variables of the code weren‚Äôt explained clearly, so I redefine it here: 1 epoch: converted to #iterations (= training_size / batch_size). 1 cycle: LR goes from base_lr -&amp;gt; max_lr -&amp;gt; base_lr. epochCounter: current #iterations (e.g: 1 epoch has 2000 iters, the model‚Äôs at epochs 1.5 =&amp;gt; epochCounter = 3000). stepsize: #iterations to reach 1/2 cycle. opt.LR: base lr cycle: which cycle the model‚Äôs currently in (1-2-‚Ä¶), always starts from 1 (first cycle). x: which part of the half-cycle, in range [0,1]. lr: the updated lr, with triangular method. Conclusions The author‚Äôs conclusions All experiments show similar or better accuracy performance when using CLR versus using a fixed learning rate, even though the performance drops at some of the learning rate values within this range. Rating Noice My Conclusion This paper is straightforward, well-explained, so I highly recommend new DL-practitioners to try reading it. CLR is an impressive technique to control the learning rate. We should try this method when we started a new model or a new dataset, giving a lovely baseline for further optimization. CLR is also widely used by kagglers. Paper implementation Cyclical Learning Rates triangular implementation: triangular_2 implementation: exp_range implementation: Plot: Explaination for how x is calculated: \(raw\_x = \frac{current\_iteration}{step\_size}\): current cycle in term of half-cycles (floatting point) \(x‚Äô = raw\_x - 2*cycle\): how many half-cycles left to complete this cycle \(x‚Äô = x‚Äô + 1\): Shift so that the function 0-centered on the y-axis, so that when we take absolute, we can achieve cycles. Learning Rate Finder An implementation of LRFinder using CLR using Callback (highly influenced by fastai): Cited references and used images from: https://www.jeremyjordan.me/nn-learning-rate/ https://github.com/bckenstler/CLR/ Papers needs to conquer next üëèüëèüëè SGDR Fit One Cycle</summary></entry><entry><title type="html">First blog</title><link href="https://trung-dt.com/talk/2020/08/01/first_blog.html" rel="alternate" type="text/html" title="First blog" /><published>2020-08-01T00:00:00+07:00</published><updated>2020-08-01T00:00:00+07:00</updated><id>https://trung-dt.com/talk/2020/08/01/first_blog</id><content type="html" xml:base="https://trung-dt.com/talk/2020/08/01/first_blog.html">&lt;p&gt;Hello.
This is my very first blog.
Lorem ipsum?&lt;/p&gt;</content><author><name></name></author><category term="talk" /><summary type="html">Hello. This is my very first blog. Lorem ipsum?</summary></entry></feed>